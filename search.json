[
  {
    "objectID": "regularization.html",
    "href": "regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Reshape and cast the input data\nx = x.view(-1, 784)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 784])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 784])\n\n\n\nTrong đoạn mã trên, tôi đã giữ y nguyên như đoạn code trong chương trước."
  },
  {
    "objectID": "regularization.html#previous-chapter-code",
    "href": "regularization.html#previous-chapter-code",
    "title": "Regularization",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Reshape and cast the input data\nx = x.view(-1, 784)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 784])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 784])\n\n\n\nTrong đoạn mã trên, tôi đã giữ y nguyên như đoạn code trong chương trước."
  },
  {
    "objectID": "regularization.html#gpu-implementation",
    "href": "regularization.html#gpu-implementation",
    "title": "Regularization",
    "section": "GPU Implementation",
    "text": "GPU Implementation\n\nimport torch\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice\n\ndevice(type='cuda')\n\n\nCUDA (Compute Unified Device Architecture): đề cập đến một nền tảng tính toán song song được phát triển bởi NVIDIA, một trong những nhà sản xuất card đồ họa và GPU lớn nhất trên thế giới. CUDA cho phép bạn sử dụng GPU (Graphics Processing Unit) của NVIDIA để thực hiện các tính toán không chỉ liên quan đến đồ họa, mà còn bao gồm các tác vụ tính toán tổng quát, bao gồm cả tính toán khoa học và máy học.\n\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        # Assume 3 hidden layers, each with 200 units\n        self.input = nn.Linear(784, 200)\n        self.hd1 = nn.Linear(200, 200)\n        self.hd2 = nn.Linear(200, 200)\n        self.hd3 = nn.Linear(200, 10)\n\n        self.layers = [self.input, self.hd1, self.hd2]\n\n    def forward(self, x):\n        # Forward pass\n        for layer in self.layers:\n            x = layer(x)\n            # Apply ReLU Activation Function\n            x = torch.relu(x)\n\n        out = self.hd3(x)\n\n        return out\n\ndef train_model(model, train_loader, valid_loader, \\\n                loss_fn, optimizer_algorithm, lr, n_epochs, device):\n\n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn(model.parameters(), lr=lr)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n\n            ### New code\n            # Convert x, y to GPU\n            x = x.to(device)\n            y = y.to(device)\n            ############\n\n            out = model.forward(x)\n\n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n\n                    ### New code\n                    # Convert x, y to GPU\n                    x = x.to(device)\n                    y = y.to(device)\n                    ##########\n\n                    out = model.forward(x)\n\n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nmodel = Model()\n\n### New code\n# Conver model to GPU\nmodel = model.to(device)\nmodel\n################\n\nModel(\n  (input): Linear(in_features=784, out_features=200, bias=True)\n  (hd1): Linear(in_features=200, out_features=200, bias=True)\n  (hd2): Linear(in_features=200, out_features=200, bias=True)\n  (hd3): Linear(in_features=200, out_features=10, bias=True)\n)\n\n\nChuyển đổi dữ liệu sang GPU là một quá trình đơn giản, tương tự việc bạn giao bài tập cho người bạn giỏi để làm giúp. Khi bạn đưa dữ liệu lên GPU, đó là như bạn đang sử dụng sức mạnh tính toán của GPU để thực hiện các phép tính. Khi bạn muốn sử dụng kết quả, bạn có thể dễ dàng đưa dữ liệu từ GPU trở lại CPU, giống như việc bạn lấy bài tập đã làm xong từ người bạn học giỏi để nộp cho giáo viên vậy.\n\nimport time\nstart_time = time.time()\n\n# Define the loss function (CrossEntropyLoss) and optimizer algorithm (Adam)\nloss_fn = nn.CrossEntropyLoss()\noptimizer_algorithm = \"Adam\"\n\n# Set the learning rate and number of training epochs\nlearning_rate = 0.01\nn_epochs = 5\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             loss_fn, \\\n                                                             optimizer_algorithm, \\\n                                                             learning_rate, \\\n                                                             n_epochs, \\\n                                                             device)\n\nend_time = time.time()\nexecution_time = end_time - start_time\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2)\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n\nModel training time: 5 min 3.26s\n\n\nNhư bạn có thể thấy, tôi đã đơn giản là sao chép code từ chương trước và chỉ thêm vài dòng code mới để chuyển x, y và model sang GPU. Nhưng điều đáng chú ý là thời gian huấn luyện mô hình đã giảm gần như một nửa. Điều này cho thấy sức mạnh của việc sử dụng GPU trong Deep Learning.\nTrước đây, khi thực hiện huấn luyện mô hình trên CPU, quá trình này thường mất rất nhiều thời gian. Nhưng với sự hỗ trợ của GPU, các phép tính toán được thực hiện nhanh chóng hơn nhiều lần, giúp cho việc phát triển Deep Learning trở nên mạnh mẽ và hiệu quả hơn.\n\nimport matplotlib.pyplot as plt\n\nfinal_loss = losses.mean(1)[-1]\nfinal_train_acc = train_acc.mean(1)[-1]\nfinal_valid_acc = valid_acc.mean(1)[-1]\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 3))\n\naxs[0].plot(range(losses.shape[0]), losses.mean(1), \"-o\")\naxs[0].set_title(f\"Train Loss is: {final_loss:.4f}\")\n\n\naxs[1].plot(range(train_acc.shape[0]), train_acc.mean(1), \"-o\")\naxs[1].plot(range(valid_acc.shape[0]), valid_acc.mean(1), \"-o\")\naxs[1].set_title(f\"Train: {final_train_acc:.2f}%, Valid: {final_valid_acc:.2f}%\")\naxs[1].legend([\"Train\", \"Valid\"])\n\nplt.suptitle(f\"Result with Neural Net in GPU\", fontsize = 16)\nplt.show()\n\n\n\n\nNhư bạn có thể thấy, những gì tôi vừa thực hiện đơn giản chỉ là việc di chuyển dữ liệu x, y và mô hình sang GPU để tận dụng sức mạnh tính toán của nó. Phần còn lại chỉ đơn giản là sao chép code từ chương trước, vì vậy kết quả mà chúng ta thu được khá tương đồng.\nBây giờ, hãy cùng tôi khám phá các kỹ thuật regularization để cải thiện độ chính xác của mô hình.\nRegularization là tập hợp các kỹ thuật được sử dụng trong deep learning để đảm bảo mô hình học từ dữ liệu một cách tổng quát hơn, thay vì chỉ học thuộc lòng các ví dụ cụ thể. Để hình dung điều này, hãy tưởng tượng bạn nhận được một đề cương ôn thi từ giáo viên. Thay vì chỉ đọc đáp án của đề cương mà không hiểu tại sao đáp án đó lại đúng, regularization giống như việc bạn muốn hiểu sâu hơn về nguyên tắc và kiến thức chung, giúp bạn tự tin giải quyết các câu hỏi khác nhau khi đi thi.\nCó 3 loại Regularization phổ biến trong Deep Learning đó là:\n\nDropout\nL2 Regularization\nBatch Normalization"
  },
  {
    "objectID": "regularization.html#dropout",
    "href": "regularization.html#dropout",
    "title": "Regularization",
    "section": "Dropout",
    "text": "Dropout\n\nimport torch.nn as nn\n\ndropout_rate = 0.6\n\ndropout_layer = nn.Dropout(p=dropout_rate)\n\nDropout là một trong những kỹ thuật regularization phổ biến trong mạng neural. Bạn có thể tưởng tượng nó như việc đôi khi bạn “ẩn đi” một số phần kiến thức quan trọng trong sách giáo trình. Điều này đặt ra một thách thức cho bạn, buộc bạn phải học và hiểu sâu hơn về nguyên tắc tổng quan của chủ đề, thay vì chỉ ghi nhớ các chi tiết cụ thể. Khi bạn đối mặt với các tình huống mới, khả năng tổng quan hóa và sáng tạo của bạn được thúc đẩy. Tương tự, trong mạng neural, dropout là việc tắt ngẫu nhiên một số neuron trong các lớp ẩn, làm cho mô hình không thể quá phụ thuộc vào các neuron cụ thể. Kỹ thuật này giúp mô hình học được các đặc trưng tổng quan của dữ liệu và tạo ra một mô hình khái quát hóa tốt hơn.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n\n        ### New Code\n        # Droput Rate\n        self.dropout_rate = dropout_rate\n        #########\n\n        # Assume 3 hidden layers, each with 200 units\n        self.input = nn.Linear(784, 200)\n        self.hd1 = nn.Linear(200, 200)\n        self.hd2 = nn.Linear(200, 200)\n        self.hd3 = nn.Linear(200, 10)\n\n        self.layers = [self.input, self.hd1, self.hd2]\n\n    def forward(self, x):\n        # Forward pass\n        for layer in self.layers:\n            x = layer(x)\n            # Apply ReLU Activation Function\n            x = torch.relu(x)\n\n            ### New Code\n            # Add droput to hidden layer\n            x = F.dropout(x, self.dropout_rate, training=self.training)\n            ##################\n\n        out = self.hd3(x)\n\n        return out\n\ndef train_model(model, train_loader, valid_loader, \\\n                loss_fn, optimizer_algorithm, lr, n_epochs, device):\n\n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn(model.parameters(), lr=lr)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n\n            ### New code\n            # Set Model to train mode\n            model.train()\n            ######\n\n            # Convert x, y to GPU\n            x = x.to(device)\n            y = y.to(device)\n\n            out = model.forward(x)\n\n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            ### New code\n            # Set Model to evaluation mode\n            model.eval()\n            ######\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    # Convert x, y to GPU\n                    x = x.to(device)\n                    y = y.to(device)\n\n                    out = model.forward(x)\n\n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nn_epochs = 1\ndropout_rate = .5\nmodel = Model(dropout_rate).to(device)\n\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             loss_fn, \\\n                                                             optimizer_algorithm, \\\n                                                             learning_rate, \\\n                                                             n_epochs,\\\n                                                             device)\n\nKhi sử dụng Dropout trong mô hình, ta cần thêm hai dòng mã để xác định mô hình đang ở chế độ huấn luyện (train mode) hoặc chế độ đánh giá (evaluation mode). Điều này quan trọng vì Dropout nên chỉ được áp dụng trong quá trình huấn luyện mô hình mà không nên ảnh hưởng đến quá trình đánh giá mô hình với dữ liệu kiểm tra.\nCũng giống như việc bạn đang ôn tập để cương. Trong quá trình ôn, bạn có thể tạm thời che đi một số thông tin hoặc ẩn đi các đáp án cụ thể để hiểu rõ hơn về chủ đề. Nhưng khi bạn bước vào kỳ thi, bạn không cần che giấu hoặc ẩn đi bất kỳ thông tin nào nữa. Lúc này, bạn cần sử dụng tất cả kiến thức mà bạn đã học để cố gắng đạt được điểm cao nhất.\nTương tự, trong mô hình neural, việc sử dụng Dropout trong quá trình huấn luyện giống như việc “che đi” một số kết nối ngẫu nhiên để ngăn chặn overfitting. Nhưng khi mô hình được đánh giá với dữ liệu kiểm tra, không cần áp dụng Dropout nữa, và mô hình sử dụng toàn bộ kiến thức để đưa ra dự đoán tốt nhất."
  },
  {
    "objectID": "regularization.html#l2-regularization",
    "href": "regularization.html#l2-regularization",
    "title": "Regularization",
    "section": "L2 Regularization",
    "text": "L2 Regularization\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Assume 3 hidden layers, each with 200 units\n        self.input = nn.Linear(784, 200)\n        self.hd1 = nn.Linear(200, 200)\n        self.hd2 = nn.Linear(200, 200)\n        self.hd3 = nn.Linear(200, 10)\n\n        self.layers = [self.input, self.hd1, self.hd2]\n\n    def forward(self, x):\n        # Forward pass\n        for layer in self.layers:\n            x = layer(x)\n            # Apply ReLU Activation Function\n            x = torch.relu(x)\n\n        out = self.hd3(x)\n\n        return out\n\ndef train_model(model, train_loader, valid_loader, \\\n                loss_fn, optimizer_algorithm, lr, n_epochs, device, weight_decay):\n\n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n\n    ### New code\n    # L2 regularization\n    optimizer = opt_fn(model.parameters(), lr=lr, weight_decay = weight_decay)\n    #####\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n            # Convert x, y to GPU\n            x = x.to(device)\n            y = y.to(device)\n\n            out = model.forward(x)\n\n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    # Convert x, y to GPU\n                    x = x.to(device)\n                    y = y.to(device)\n\n                    out = model.forward(x)\n\n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nn_epochs = 1\nweight_decay = .0001\n\nmodel = Model().to(device)\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             loss_fn, \\\n                                                             optimizer_algorithm, \\\n                                                             learning_rate, \\\n                                                             n_epochs,\\\n                                                             device, \\\n                                                             weight_decay)\n\nCông thức L2 regularization được sử dụng để kiểm soát giá trị tuyệt đối của trọng số (weight) trong mô hình và thêm nó vào hàm mất mát (loss). Công thức này có dạng:\n\\[\nL2\\_Loss = \\frac{\\lambda}{2} \\sum_{i=1}^{n} w_i^2\n\\]\nTrong đó: - \\({\\lambda}\\): là tham số “weight decay” (điều chỉnh mức độ của regularization).\n\n\\({n}\\) là số lượng trọng số (weight) trong mô hình.\n\\({w_i}\\) là trọng số thứ \\(i\\) của mô hình.\n\nL2 regularization sẽ thêm một tham số gọi là “weight decay” vào hàm mất mát (loss function) của mô hình. Thay vì để mô hình tự do tạo ra các trọng số có giá trị lớn và phức tạp, L2 regularization áp đặt một giới hạn để kiểm soát giá trị tuyệt đối của chúng.\nHãy tưởng tượng rằng bạn đang ôn thi và cần học một đề cương phức tạp. Thay vì nỗ lực học thuộc lòng từng phần chi tiết của đề cương, bạn quyết định tập trung vào các khái niệm và nguyên tắc tổng quan. Bạn thiết lập một giới hạn cho mức độ chi tiết bạn nên nắm vững, giúp bạn hiểu sâu hơn về chủ đề mà không bị lạc hướng vào những thông tin không quan trọng. Tương tự, L2 regularization giúp mô hình tập trung vào các đặc trưng quan trọng và kiểm soát sự phụ thuộc vào những chi tiết không cần thiết, đồng thời giúp ngăn chặn hiện tượng overfitting (học thuộc lòng đáp án) trong quá trình học."
  },
  {
    "objectID": "regularization.html#batch-normalization",
    "href": "regularization.html#batch-normalization",
    "title": "Regularization",
    "section": "Batch Normalization",
    "text": "Batch Normalization\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Assume 3 hidden layers, each with 200 units\n        self.input = nn.Linear(784, 200)\n        self.hd1 = nn.Linear(200, 200)\n        self.hd2 = nn.Linear(200, 200)\n        self.hd3 = nn.Linear(200, 10)\n\n        ### New Code\n        # Batch Normalization layers\n        self.bn1 = nn.BatchNorm1d(200)\n        self.bn2 = nn.BatchNorm1d(200)\n\n        self.layers = [self.input, self.bn1,self.hd1, self.bn2, self.hd2]\n        #########\n\n    def forward(self, x):\n        # Forward pass\n        for layer in self.layers:\n            x = layer(x)\n            # Apply ReLU Activation Function\n            x = torch.relu(x)\n\n        out = self.hd3(x)\n\n        return out\n\ndef train_model(model, train_loader, valid_loader, \\\n                loss_fn, optimizer_algorithm, lr, n_epochs, device):\n\n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn(model.parameters(), lr=lr)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n            # Convert x, y to GPU\n            x = x.to(device)\n            y = y.to(device)\n\n            out = model.forward(x)\n\n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    # Convert x, y to GPU\n                    x = x.to(device)\n                    y = y.to(device)\n\n                    out = model.forward(x)\n\n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nn_epochs = 1\nmodel = Model().to(device)\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             loss_fn, \\\n                                                             optimizer_algorithm, \\\n                                                             learning_rate, \\\n                                                             n_epochs,\\\n                                                             device)\n\n\nBatch Normalization (BatchNorm) là một kỹ thuật quan trọng trong deep learning. Nó ra đời để giải quyết vấn đề trong quá trình huấn luyện mô hình, khi đầu ra của các lớp ẩn có thể trở nên quá lớn hoặc quá nhỏ, gây khó khăn cho quá trình học của mô hình. BatchNorm thực hiện chuẩn hóa đầu ra của các lớp ẩn, bằng cách đảm bảo rằng giá trị trung bình gần bằng 0 và độ lệch chuẩn gần bằng 1, ngay cả khi dữ liệu đầu vào không tuân theo phân phối chuẩn (Gaussian distribution).\nVí dụ: Trong mạng neural thông thường, đầu ra của các lớp ẩn có thể thay đổi theo từng mini-batch của dữ liệu huấn luyện. BatchNorm đảm bảo rằng các đầu ra này có giá trị trung bình gần bằng 0 và độ lệch chuẩn gần bằng 1, giúp mô hình học nhanh hơn và ổn định hơn.\nMột điểm quan trọng cần lưu ý là BatchNorm không thường được sử dụng ở lớp ẩn cuối cùng trước khi đầu ra được đưa qua trọng số của lớp output (chỗ được đúng khung màu xanh dương). Điều này giúp duy trì tính tự nhiên của đầu ra của mô hình trong các tác vụ như phân loại, trong đó phân phối của đầu ra có thể không cần phải được chuẩn hóa.\nTóm lại, Batch Normalization chuẩn hóa đầu ra của các lớp ẩn trong mô hình, đảm bảo rằng chúng luôn có giá trị trung bình gần bằng 0 và độ lệch chuẩn gần bằng 1. Điều này giúp tăng tốc quá trình huấn luyện và làm cho mô hình ổn định hơn trong deep learning."
  },
  {
    "objectID": "regularization.html#altogether",
    "href": "regularization.html#altogether",
    "title": "Regularization",
    "section": "Altogether",
    "text": "Altogether\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, dropout_rate):\n        super().__init__()\n\n        ### New Code\n        # Droput Rate\n        self.dropout_rate = dropout_rate\n        #########\n\n        # Assume 3 hidden layers, each with 200 units\n        self.input = nn.Linear(784, 200)\n        self.hd1 = nn.Linear(200, 200)\n        self.hd2 = nn.Linear(200, 200)\n        self.hd3 = nn.Linear(200, 10)\n\n        ### New Code\n        # Batch Normalization layers\n        self.bn1 = nn.BatchNorm1d(200)\n        self.bn2 = nn.BatchNorm1d(200)\n\n        self.layers = [self.input, self.bn1,self.hd1, self.bn2, self.hd2]\n        #########\n\n    def forward(self, x):\n        # Forward pass\n        for layer in self.layers:\n            x = layer(x)\n            # Apply ReLU Activation Function\n            x = torch.relu(x)\n\n            ### New Code\n            # Add droput to hidden layer\n            x = F.dropout(x, self.dropout_rate, training=self.training)\n            ##################\n\n        out = self.hd3(x)\n\n        return out\n\ndef train_model(model, train_loader, valid_loader, \\\n                loss_fn, optimizer_algorithm, lr, n_epochs, device, weight_decay):\n\n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n\n    ### New code\n    # L2 regularization\n    optimizer = opt_fn(model.parameters(), lr=lr, weight_decay = weight_decay)\n    #####\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n\n            ### New code\n            # Set Model to train mode\n            model.train()\n            ######\n\n            # Convert x, y to GPU\n            x = x.to(device)\n            y = y.to(device)\n\n            out = model.forward(x)\n\n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            ### New code\n            # Set Model to evaluation mode\n            model.eval()\n            ######\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    # Convert x, y to GPU\n                    x = x.to(device)\n                    y = y.to(device)\n\n                    out = model.forward(x)\n\n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nimport time\nstart_time = time.time()\n\nn_epochs = 5\ndroput_rate = .1\nweight_decay = .0001\nlearning_rate = .01\n\nmodel = Model(droput_rate).to(device)\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             loss_fn, \\\n                                                             optimizer_algorithm, \\\n                                                             learning_rate, \\\n                                                             n_epochs,\\\n                                                             device, \\\n                                                             weight_decay)\n\nend_time = time.time()\nexecution_time = end_time - start_time\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2)\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n\nModel training time: 5 min 48.39s\n\n\n\nimport matplotlib.pyplot as plt\n\nfinal_loss = losses.mean(1)[-1]\nfinal_train_acc = train_acc.mean(1)[-1]\nfinal_valid_acc = valid_acc.mean(1)[-1]\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 3))\n\naxs[0].plot(range(losses.shape[0]), losses.mean(1), \"-o\")\naxs[0].set_title(f\"Train Loss is: {final_loss:.4f}\")\n\n\naxs[1].plot(range(train_acc.shape[0]), train_acc.mean(1), \"-o\")\naxs[1].plot(range(valid_acc.shape[0]), valid_acc.mean(1), \"-o\")\naxs[1].set_title(f\"Train: {final_train_acc:.2f}%, Valid: {final_valid_acc:.2f}%\")\naxs[1].legend([\"Train\", \"Valid\"])\n\nplt.suptitle(f\"Result with NN + Regularization\", fontsize = 16)\nplt.show()\n\n\n\n\nKết quả đã không tăng mà còn giảm đi nữa. Hãy cùng xem xét liệu có thể có kết quả tích cực hơn đối với tập dữ liệu kiểm tra không nhé!\n\nx_test = test_dataset.data\ny_test = test_dataset.targets\n\n# Reshape and cast the input data\nx_test = x_test.view(-1, 784)\nx_test = x_test.to(torch.float32)\nx_test_norm = min_max_scale(x_test)\n\n# Cast the target labels to the 'long' data type\ny_test = y_test.to(torch.long)\n\n\nmodel.eval()\nout = model.forward(x_test_norm.to(device)).cpu()\nmodel.train()\n\ntest_acc = 100 * (out.argmax(1) == y_test).float().mean()\n\nids_error = (out.argmax(1) != y_test).nonzero()\nprint(f\"In {len(y_test)} images, model guess wrong {len(ids_error)}\")\nprint(f\"Accuracy is: {test_acc:.2f}%\")\n\nIn 10000 images, model guess wrong 1883\nAccuracy is: 81.17%\n\n\nKết quả đã giảm so với khi không sử dụng regularization, nhưng điều này có thể là một kết quả hợp lý. Regularization thường được sử dụng để ngăn chặn hiện tượng overfitting, tức là mô hình quá tinh chỉnh để phù hợp với dữ liệu huấn luyện một cách quá mức. Tuy nhiên, trong trường hợp của chúng ta, mô hình ban đầu đã chỉ đạt đến mức độ chính xác gần 85% trên tập dữ liệu huấn luyện. Do đó, việc áp dụng regularization có thể khiến mô hình trở nên quá cứng rắn và không thể học được từ dữ liệu huấn luyện một cách hiệu quả, dẫn đến giảm hiệu suất trên tập dữ liệu kiểm tra.\nHãy tiếp tục hành trình với việc áp dụng kỹ thuật regularization lên Convolutional Neural Network (CNN). Chúng ta sẽ khám phá xem liệu việc này có thể mang lại kết quả tích cực hơn không. Chương tiếp theo sẽ giúp chúng ta tiếp tục khám phá những khả năng và tiềm năng của mô hình này. Hãy cùng nhau đón xem và tiến sâu vào nghiên cứu trong chương tiếp theo!\n\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\n\nfor ax in axs.flatten():\n    # Select a random index from the output\n    random_index = np.random.choice(len(out))\n\n    # Display the image at the selected index with colormap gray for correct predictions\n    true_label = y_test[random_index]\n    guess_label = out.argmax(1)[random_index]\n\n    if true_label == guess_label:\n        ax.imshow(x_test[random_index].view(28, 28), cmap=\"gray\")\n    else:\n        ax.imshow(x_test[random_index].view(28, 28))\n\n\n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Result\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "cnn.html",
    "href": "cnn.html",
    "title": "CNN",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Reshape and cast the input data\nx = x.view(-1, 784)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 784])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 784])"
  },
  {
    "objectID": "cnn.html#previous-chapter-code",
    "href": "cnn.html#previous-chapter-code",
    "title": "CNN",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Reshape and cast the input data\nx = x.view(-1, 784)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 784])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 784])"
  },
  {
    "objectID": "cnn.html#cnn",
    "href": "cnn.html#cnn",
    "title": "CNN",
    "section": "CNN",
    "text": "CNN\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef calculate_output_size(input_size, kernel_size, padding=0, stride=1):\n    output_size = np.floor((input_size - kernel_size + 2 * padding) / stride) + 1\n    return int(output_size)\n\nclass CustomCNN(nn.Module):\n    def __init__(self, dropout_rate):\n        super(CustomCNN, self).__init__()\n\n        kernel_size = 3\n        padding = 1\n        image_size = 28\n        num_channels1 = 64\n        num_channels2 = 128\n\n        # Calculate the output size after each convolution and pooling\n        output_size = calculate_output_size(image_size, kernel_size, padding)\n        output_size = int(np.floor(output_size / 2))\n\n        output_size = calculate_output_size(output_size, kernel_size)\n        output_size = int(np.floor(output_size / 2))\n\n        input_features = output_size * output_size * num_channels2\n\n        # Convolutional layers with BatchNorm and MaxPool\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, num_channels1, kernel_size, padding=padding),\n            nn.LeakyReLU(),\n            nn.BatchNorm2d(num_channels1),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(num_channels1, num_channels2, kernel_size),\n            nn.LeakyReLU(),\n            nn.BatchNorm2d(num_channels2),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n        )\n\n        self.dropout_rate = dropout_rate\n        # Fully connected layers with BatchNorm\n        self.fc1 = nn.Linear(input_features, 100)\n        self.fc2 = nn.Linear(100, 100)\n        self.fc3 = nn.Linear(100, 10)\n\n        self.bn1 = nn.BatchNorm1d(100)\n\n        self.hidden_layers = [self.fc1, self.bn1, self.fc2]\n\n    def forward(self, x):\n        x = x.view(-1, 1,28, 28)\n\n        conv = self.conv(x)\n        x = conv.flatten(1)\n        for layer in self.hidden_layers:\n            x = layer(x)\n            x = F.relu(x)  # ReLU activation applied after BatchNorm\n            x = F.dropout(x, self.dropout_rate, training=self.training)\n\n        output = self.fc3(x)\n        return output\n\n# Instantiate the model with a dropout rate\nmodel = CustomCNN(dropout_rate=0.5)\n\n\ndef train_model(model, train_loader, valid_loader, \\\n                lr, n_epochs, device, weight_decay):\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n\n            # Set Model to train mode\n            model.train()\n\n            out = model.forward(x.to(device))\n\n            # Calculate loss\n            loss = loss_fn(out, y.to(device))\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Set Model to evaluation mode\n            model.eval()\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    out = model.forward(x.to(device))\n\n                    acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nimport time\nstart_time = time.time()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nn_epochs = 5\ndroput_rate = .1\nweight_decay = .0001\nlearning_rate = .01\n\nmodel = CustomCNN(droput_rate).to(device)\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             learning_rate, \\\n                                                             n_epochs,\\\n                                                             device, \\\n                                                             weight_decay)\nend_time = time.time()\nexecution_time = end_time - start_time\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2)\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n\nModel training time: 9 min 58.41s\n\n\n\nimport matplotlib.pyplot as plt\n\nfinal_loss = losses.mean(1)[-1]\nfinal_train_acc = train_acc.mean(1)[-1]\nfinal_valid_acc = valid_acc.mean(1)[-1]\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 3))\n\naxs[0].plot(range(losses.shape[0]), losses.mean(1), \"-o\")\naxs[0].set_title(f\"Train Loss is: {final_loss:.4f}\")\n\n\naxs[1].plot(range(train_acc.shape[0]), train_acc.mean(1), \"-o\")\naxs[1].plot(range(valid_acc.shape[0]), valid_acc.mean(1), \"-o\")\naxs[1].set_title(f\"Train: {final_train_acc:.2f}%, Valid: {final_valid_acc:.2f}%\")\naxs[1].legend([\"Train\", \"Valid\"])\n\nplt.suptitle(f\"Result with CNN\", fontsize = 16)\nplt.show()\n\n\n\n\n\nx_test = test_dataset.data\ny_test = test_dataset.targets\n\n# Reshape and cast the input data\nx_test = x_test.view(-1, 784)\nx_test = x_test.to(torch.float32)\nx_test_norm = min_max_scale(x_test)\n\n# Cast the target labels to the 'long' data type\ny_test = y_test.to(torch.long)\n\ntest_loader = load_data(x_test_norm, y_test, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 79\nShape of Each Mini-Batch: torch.Size([128, 784])\n\n\n\n\ntest_acc = []\nmodel.eval()\nfor i, (x, y) in enumerate(test_loader):\n    out = model.forward(x.to(device))\n\n    acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n    test_acc.append(acc.cpu())\nmodel.train()\n\nprint(f\"Accuracy is: {np.mean(test_acc):.2f}%\")\n\nAccuracy is: 87.77%"
  },
  {
    "objectID": "cnn.html#save-model",
    "href": "cnn.html#save-model",
    "title": "CNN",
    "section": "Save Model",
    "text": "Save Model\n\n# Save Model\nfrom google.colab import drive\ndrive.mount('/content/drive')\nmodel_path = '/content/drive/MyDrive/fnmnist_model.pth'\ntorch.save(model.state_dict(), model_path)\n\nMounted at /content/drive\n\n\n\n# Load the model\nmodel = CustomCNN(.1)\nmodel.load_state_dict(torch.load('/content/drive/MyDrive/fnmnist_model.pth',  map_location=torch.device('cpu')))\n\n&lt;All keys matched successfully&gt;\n\n\n\ntest_acc = []\nmodel.to(device)\nmodel.eval()\nfor i, (x, y) in enumerate(test_loader):\n    out = model.forward(x.to(device))\n\n    acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n    test_acc.append(acc.cpu())\nmodel.train()\n\nprint(f\"Accuracy is: {np.mean(test_acc):.2f}%\")\n\nAccuracy is: 87.77%\n\n\n\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\n\nfor ax in axs.flatten():\n    # Select a random index from the output\n    random_index = np.random.choice(len(out))\n\n    # Display the image at the selected index with colormap gray for correct predictions\n    true_label = y_test[random_index]\n    guess_label = out.argmax(1)[random_index]\n\n    if true_label == guess_label:\n        ax.imshow(x_test[random_index].view(28, 28), cmap=\"gray\")\n    else:\n        ax.imshow(x_test[random_index].view(28, 28))\n\n\n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Result\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "process.html",
    "href": "process.html",
    "title": "Process Data",
    "section": "",
    "text": "import torchvision\nfrom torchvision import datasets\n# Download Data\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# Check the lengths of the datasets\nprint(\"Number of samples in the training set:\", len(train_dataset))\nprint(\"Number of samples in the test set:\", len(test_dataset))\n\nNumber of samples in the training set: 60000\nNumber of samples in the test set: 10000\n\n\nFashion MNIST là một bộ dữ liệu chứa các hình ảnh về sản phẩm thời trang từ Zalando, bao gồm một bộ dữ liệu huấn luyện (training set) với 60,000 ví dụ và một bộ dữ liệu kiểm tra (test set) với 10,000 ví dụ.\n\nx = train_dataset.data\nprint(f\"Shape: {x.shape}\")\nprint(f\"Min: {x.min().item()}\")\nprint(f\"Max: {x.max().item()}\")\n\nShape: torch.Size([60000, 28, 28])\nMin: 0\nMax: 255\n\n\nMỗi hình ảnh có chiều cao 28 pixel và chiều rộng 28 pixel, tổng cộng có 784 pixel. Giá trị pixel này là một số nguyên nằm trong khoảng từ 0 đến 255.\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\ny = train_dataset.targets\nprint(f\"Labels: {y.unique()}\")\n\nLabels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nMỗi ví dụ trong bộ dữ liệu huấn luyện và bộ dữ liệu kiểm tra được gán cho một trong các nhãn (label) sau:\n\n0 Áo phông/T-shirt\n1 Quần dài/Trouser\n2 Áo khoác len/Pullover\n3 Váy/Dress\n4 Áo khoác/Coat\n5 Dép xỏ ngón/Sandal\n6 Áo sơ mi/Shirt\n7 Giày thể thao/Sneaker\n8 Túi xách/Bag\n9 Giày bốt/Cổ cao/Ankle boot\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 8, figsize = (14, 6))\nfor ax in axs.flatten():\n\n    random_index = np.random.randint(len(x))\n\n    ax.imshow(x[random_index, :], cmap = \"gray\")\n\n    ax.set_title(f\"{classes[y[random_index].item()]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Training data\", fontsize = 18)\nplt.show()"
  },
  {
    "objectID": "process.html#about-fashion-mnist",
    "href": "process.html#about-fashion-mnist",
    "title": "Process Data",
    "section": "",
    "text": "import torchvision\nfrom torchvision import datasets\n# Download Data\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# Check the lengths of the datasets\nprint(\"Number of samples in the training set:\", len(train_dataset))\nprint(\"Number of samples in the test set:\", len(test_dataset))\n\nNumber of samples in the training set: 60000\nNumber of samples in the test set: 10000\n\n\nFashion MNIST là một bộ dữ liệu chứa các hình ảnh về sản phẩm thời trang từ Zalando, bao gồm một bộ dữ liệu huấn luyện (training set) với 60,000 ví dụ và một bộ dữ liệu kiểm tra (test set) với 10,000 ví dụ.\n\nx = train_dataset.data\nprint(f\"Shape: {x.shape}\")\nprint(f\"Min: {x.min().item()}\")\nprint(f\"Max: {x.max().item()}\")\n\nShape: torch.Size([60000, 28, 28])\nMin: 0\nMax: 255\n\n\nMỗi hình ảnh có chiều cao 28 pixel và chiều rộng 28 pixel, tổng cộng có 784 pixel. Giá trị pixel này là một số nguyên nằm trong khoảng từ 0 đến 255.\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\ny = train_dataset.targets\nprint(f\"Labels: {y.unique()}\")\n\nLabels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n\nMỗi ví dụ trong bộ dữ liệu huấn luyện và bộ dữ liệu kiểm tra được gán cho một trong các nhãn (label) sau:\n\n0 Áo phông/T-shirt\n1 Quần dài/Trouser\n2 Áo khoác len/Pullover\n3 Váy/Dress\n4 Áo khoác/Coat\n5 Dép xỏ ngón/Sandal\n6 Áo sơ mi/Shirt\n7 Giày thể thao/Sneaker\n8 Túi xách/Bag\n9 Giày bốt/Cổ cao/Ankle boot\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 8, figsize = (14, 6))\nfor ax in axs.flatten():\n\n    random_index = np.random.randint(len(x))\n\n    ax.imshow(x[random_index, :], cmap = \"gray\")\n\n    ax.set_title(f\"{classes[y[random_index].item()]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Training data\", fontsize = 18)\nplt.show()"
  },
  {
    "objectID": "process.html#split-data",
    "href": "process.html#split-data",
    "title": "Process Data",
    "section": "Split data",
    "text": "Split data\n\nlabels, counts = np.unique(y, return_counts=True)\n\nprint(\"Checking Data Balance:\")\nfor label, count in zip(labels, counts):\n    print(f\" Label: {label}, Count: {count}\")\n\nChecking Data Balance:\n Label: 0, Count: 6000\n Label: 1, Count: 6000\n Label: 2, Count: 6000\n Label: 3, Count: 6000\n Label: 4, Count: 6000\n Label: 5, Count: 6000\n Label: 6, Count: 6000\n Label: 7, Count: 6000\n Label: 8, Count: 6000\n Label: 9, Count: 6000\n\n\nDữ liệu đã được cân bằng hoàn hảo, với mỗi nhãn (label) chứa đúng 6000 mẫu dữ liệu.\nĐể hiểu điều này, hãy xem xét một ví dụ tương tự trong cuộc thi có 10 câu hỏi với 4 phương án trả lời A, B, C, D. Trong tình huống này, số câu trả lời đúng cho mỗi phương án là 8 câu A, 1 câu B, 0 câu C và 1 câu D.\nGiả sử bạn được phép làm cuộc thi này nhiều lần. Sau một thời gian, bạn sẽ nhận thấy một sự thiên vị trong số câu trả lời đúng cho mỗi phương án. Điều này có nghĩa rằng câu A thường là câu trả lời đúng và bạn có thể dễ dàng chọn câu A cho tất cả các câu hỏi để đạt điểm số cao.\nTương tự như vậy, trong việc kiểm tra độ cân bằng của dữ liệu, chúng ta đảm bảo rằng mô hình deep learning được đào tạo trên dữ liệu mẫu đại diện cho tất cả các trường hợp, thay vì bị thiên vị theo một phương án hoặc lớp cụ thể. Điều này giúp đảm bảo tính công bằng và độ chính xác của mô hình trong ứng dụng thực tế.\n\n# Split the data into training and validation sets\n\n# Calculate the number of validation samples per label\nnum_labels = len(np.unique(y))\nval_pct = 0.2\nval_samples_per_label = int(len(x) * val_pct / num_labels)\n\nvalid_indices = []\n\n# Iterate through each unique label\nfor label in np.unique(y):\n    # Get the indices of samples belonging to this label\n    label_indices = np.where(y == label)[0]\n    \n    # Randomly select a specified number of validation samples from this label\n    sampled_indices = np.random.choice(label_indices, val_samples_per_label, replace=False)\n    valid_indices.append(sampled_indices)\n\nexcluded_indices = np.concatenate(valid_indices)\nmask = np.ones(len(x), dtype=bool)\nmask[excluded_indices] = False\n\n# Separate the data into training and validation sets\nx_train = x[mask]\nx_valid = x[excluded_indices]\ny_train = y[mask]\ny_valid = y[excluded_indices]\n\n# Display the shapes of the training and validation sets\nx_train.shape, y_train.shape, x_valid.shape, y_valid.shape\n\n(torch.Size([48000, 28, 28]),\n torch.Size([48000]),\n torch.Size([12000, 28, 28]),\n torch.Size([12000]))\n\n\n\nval_labels, counts = np.unique(y_valid, return_counts=True)\n\nprint(\"Checking Valid Data Balance:\")\nfor val_label, count in zip(val_labels, counts):\n    print(f\" Label: {val_label}, Count: {count}\")\n\nChecking Valid Data Balance:\n Label: 0, Count: 1200\n Label: 1, Count: 1200\n Label: 2, Count: 1200\n Label: 3, Count: 1200\n Label: 4, Count: 1200\n Label: 5, Count: 1200\n Label: 6, Count: 1200\n Label: 7, Count: 1200\n Label: 8, Count: 1200\n Label: 9, Count: 1200\n\n\nTrong quá trình phát triển mô hình deep learning, việc chia dữ liệu thành hai phần: tập huấn luyện (train set) và tập xác thực (validation set) , đồng thời duy trì sự cân bằng giữa các nhãn (labels) trong cả hai tập là rất quan trọng. Điều này tương tự như việc bạn ôn tập để chuẩn bị cho một kỳ thi\nHãy tưởng tượng bạn nhận được một đề cương ôn thi gồm 20 câu hỏi, trong đó mỗi câu có 4 phương án A, B, C, D và chỉ có một phương án đúng. Đề cương này đã được thiết kế sao cho cân bằng về số lượng câu trả lời đúng cho mỗi phương án. Trong trường hợp này, bạn có hai cách để ôn:\nCách 1: Học thuộc lòng đáp án của đề cương Bạn có thể chọn học thuộc lòng đáp án cho từng câu hỏi trong đề cương. Tuy nhiên, khi tham gia kỳ thi thực tế, cách này thường dẫn đến điểm thấp, vì bạn có thể gặp những biến thể hoặc câu hỏi tương tự mà bạn chưa từng thấy trước đó. Bạn thiếu khả năng áp dụng kiến thức vào các tình huống mới và sâu hơn về nội dung thực sự của các câu hỏi.\nCách 2: Học để hiểu cách trả lời câu hỏi một cách tổng quát Thay vì chỉ học thuộc lòng đáp án, bạn cố gắng hiểu sâu về kiến thức và logic đằng sau mỗi câu hỏi. Điều này giúp bạn phát triển khả năng áp dụng kiến thức vào các tình huống mới, bao gồm những biến thể của câu hỏi mà bạn có thể gặp trong kỳ thi thực tế. Khi tham gia kỳ thi, bạn sẽ tự tin hơn và có khả năng xử lý bất kỳ câu hỏi nào, giúp bạn đạt được điểm cao hơn.\nTương tự, việc chia dữ liệu thành tập huấn luyện và tập xác thực trong deep learning giúp mô hình học cách phân loại dữ liệu và tạo ra các dự đoán không chỉ dựa trên việc ghi nhớ dữ liệu huấn luyện. Nó giúp mô hình hiểu và tổng quát hóa kiến thức, từ đó cải thiện khả năng dự đoán trên dữ liệu mới và không biết trước (tập xác thực hoặc tập kiểm tra). Việc này giúp tránh overfitting, cải thiện hiệu suất của mô hình và làm cho nó trở nên đáng tin cậy hơn trong các ứng dụng thực tế.\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\nprint(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n\nval_labels, counts = np.unique(y_valid, return_counts=True)\nprint(\"Checking Valid Data Balance:\")\nfor val_label, count in zip(val_labels, counts):\n    print(f\" Label: {val_label}, Count: {count}\")\n\ntorch.Size([48000, 28, 28]) torch.Size([48000]) torch.Size([12000, 28, 28]) torch.Size([12000])\nChecking Valid Data Balance:\n Label: 0, Count: 1206\n Label: 1, Count: 1273\n Label: 2, Count: 1219\n Label: 3, Count: 1190\n Label: 4, Count: 1163\n Label: 5, Count: 1176\n Label: 6, Count: 1197\n Label: 7, Count: 1173\n Label: 8, Count: 1210\n Label: 9, Count: 1193\n\n\nƠn trời, train_test_split thực sự đã làm cuộc sống của chúng ta trở nên dễ dàng và hạnh phúc hơn rất nhiều! 😃"
  },
  {
    "objectID": "process.html#normalization",
    "href": "process.html#normalization",
    "title": "Process Data",
    "section": "Normalization",
    "text": "Normalization\nCó ba phương pháp chuẩn hóa thường xuyên được sử dụng bao gồm:\n\nMin-Max Scaling\nZ-Score (Standardization)\nRobust Scaling.\n\n\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n    \n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n    \n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n    \n    return data_norm\n\nCông thức của Min-Max Scaling:\nMin-Max Scaling là một phương pháp chuẩn hóa dữ liệu được sử dụng để ánh xạ giá trị của một dữ liệu từ khoảng ban đầu [Min, Max] sang một khoảng mới [a, b], với a và b là giá trị cụ thể.\nCông thức để scale giá trị x của một dữ liệu từ khoảng ban đầu [Min, Max] sang khoảng [0, 1] là:\n\\[ \\text{Scaled Value} = \\frac{{\\text{x} - \\text{Min}}}{{\\text{Max} - \\text{Min}}} \\]\nĐể scale giá trị x của một dữ liệu về khoảng [a, b], công thức sẽ là:\n\\[ \\text{Scaled Value} = a + \\frac{{\\text{x} - \\text{Min}}}{{\\text{Max} - \\text{Min}}} \\cdot (b - a) \\]\nTrong đó:\n\nScale Value: Giá trị sau khi chuẩn hóa của dữ liệu theo phương pháp Min-Max Scaling.\nx: Giá trị ban đầu của dữ liệu.\nMin: Giá trị nhỏ nhất trong khoảng ban đầu.\nMax: Giá trị lớn nhất trong khoảng ban đầu.\na: Giá trị nhỏ nhất trong khoảng mới (thường là 0).\nb: Giá trị lớn nhất trong khoảng mới (thường là 1).\n\n\ndef z_score(data):\n    # Calculate the mean and standard deviation of the input data\n    data_mean = data.mean()\n    data_std = data.std(ddof=1)\n    \n    # Perform Z-score (standardization) on the data\n    data_norm = (data - data_mean) / data_std\n\n    return data_norm\n\nCông thức của Z-score (Standardization):\nZ-score (hoặc Standardization) là một phương pháp chuẩn hóa dữ liệu được sử dụng để biến đổi giá trị của một dữ liệu sao cho nó có giá trị trung bình (mean) bằng 0 và độ lệch chuẩn (standard deviation) bằng 1.\nCông thức để tính Z-score cho một giá trị x trong dữ liệu là:\n\\[ \\text{Z-score} = \\frac{{\\text{x} - \\text{Mean}}}{{\\text{Standard Deviation}}} \\]\nTrong đó:\n\nZ-score: Giá trị Z-score của dữ liệu sau khi chuẩn hóa.\nx: Giá trị ban đầu của dữ liệu.\nMean: Giá trị trung bình của dữ liệu.\nStandard Deviation: Độ lệch chuẩn của dữ liệu.\n\n\ndef robust_scale(data):\n    # Calculate the median and interquartile range (IQR) of the data\n    data_median = np.median(data)\n    iqr = np.percentile(data, 75) - np.percentile(data, 25)\n    \n    # Perform Robust Scaling\n    data_scaled = (data - data_median) / iqr\n    \n    return data_scaled\n\nCông thức của Robust Scaling:\nRobust Scaling (hoặc Robust Standardization) là một phương pháp chuẩn hóa dữ liệu được sử dụng để biến đổi giá trị của một dữ liệu sao cho nó có tính ổn định đối với sự hiện diện của các outliers. Phương pháp này sử dụng median và khoảng biến thiên (IQR - Interquartile Range) của dữ liệu để thực hiện chuẩn hóa.\nCông thức để tính Robust Scaling cho một giá trị x trong dữ liệu là:\n\\[ \\text{Robust Scaled} = \\frac{{\\text{x} - \\text{Median}}}{{\\text{IQR}}} \\]\nTrong đó:\n\nRobust Scaled: Giá trị sau khi chuẩn hóa của dữ liệu theo phương pháp Robust Scaling.\nx: Giá trị ban đầu của dữ liệu.\nMedian: Giá trị trung vị (median) của dữ liệu.\nIQR: Khoảng biến thiên giữa phân vị 25% (Q1) và phân vị 75% (Q3) của dữ liệu.\n\nLựa chọn phương pháp normalization\nViệc lựa chọn phương pháp chuẩn hóa dữ liệu phụ thuộc vào tính chất của dữ liệu và mục tiêu cụ thể của dự án. Trong trường hợp dự án Fashion MNIST, dữ liệu là hình ảnh, do đó tôi đã chọn thử nghiệm hai phương pháp chuẩn hóa: Min-Max Scaling và Robust Scaling.\nLý do là Min-Max Scaling đơn giản chỉ định khoảng giá trị cho dữ liệu mà không thay đổi tính chất của hình ảnh ban đầu. Robust Scaling cũng phù hợp vì nó sử dụng median thay vì giá trị trung bình, nên nó có khả năng xử lý tốt với các bộ dữ liệu không tuân theo phân phối Gaussian. Tuy nhiên, Z-Score sử dụng mean nên không phải lúc nào cũng phù hợp cho các dữ liệu không tuân theo phân phối Gaussian.\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\nprint(f\"Train Data: {x_train_norm.shape}, Min: {x_train_norm.min()}, Max: {x_train_norm.max()}\")\nprint(f\"Valid Data: {x_valid_norm.shape}, Min: {x_valid_norm.min()}, Max: {x_valid_norm.max()}\")\n\nTrain Data: torch.Size([48000, 28, 28]), Min: 0.0, Max: 1.0\nValid Data: torch.Size([12000, 28, 28]), Min: 0.0, Max: 1.0\n\n\nTrước hết, tôi sẽ bắt đầu bằng việc chuẩn hóa dữ liệu bằng phương pháp Min-Max Scaling. Sau đó, khi tiến đến giai đoạn tối ưu hóa mô hình để tăng độ chính xác trong việc dự đoán dữ liệu xác thực (Validation Data), tôi sẽ quay lại và thử nghiệm với Robust Scaling để xem liệu có cải thiện độ chính xác của mô hình hay không."
  },
  {
    "objectID": "process.html#load-data",
    "href": "process.html#load-data",
    "title": "Process Data",
    "section": "Load Data",
    "text": "Load Data\nTrong quá trình tải dữ liệu và huấn luyện mô hình Deep Learning, có ba khái niệm quan trọng cần hiểu:\n\nBatches:\nMini-Batches:\nStochastic Gradient Descent (SGD):\n\nLựa chọn giữa Batches, Mini-Batches và SGD phụ thuộc vào tài nguyên và mục tiêu huấn luyện của bạn. Chúng có ảnh hưởng đến tốc độ học của mô hình và khả năng tổng quát hóa.\n\ndef load_data(x_tensor, y_tensor, batch_size, drop_last=False, test=False):\n    num_samples = len(x_tensor)\n    \n    mini_batches = []\n    \n    if not test:\n        # Shuffle the data if it's the training set\n        indices = np.arange(num_samples)\n        np.random.shuffle(indices)\n        x_tensor = x_tensor[indices]\n        y_tensor = y_tensor[indices]\n        \n    for i in range(0, num_samples, batch_size):\n        # Handling the last mini-batch\n        if i + batch_size &gt; num_samples:\n            # If not dropping the last mini-batch\n            if not drop_last:\n                batch_x = x_tensor[i:num_samples]\n                batch_y = y_tensor[i:num_samples]\n        \n        # For other mini-batches\n        else:\n            batch_x = x_tensor[i:i + batch_size]\n            batch_y = y_tensor[i:i + batch_size]\n        \n        mini_batches.append((batch_x, batch_y))\n    \n    print(f\"Total Mini-Batches: {len(mini_batches)}\")\n    for i, (x, y) in enumerate(mini_batches):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    \n    return mini_batches\n\n# Batches (batch_size = length of data)\ntrain_batch_size = len(x_train_norm)\nvalid_batch_size = len(x_valid_norm)\n\ntrain_loader = load_data(x_train_norm, y_train, batch_size=train_batch_size)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=valid_batch_size, test=True)\n\nTotal Mini-Batches: 1\nShape of Each Mini-Batch: torch.Size([48000, 28, 28])\n\nTotal Mini-Batches: 1\nShape of Each Mini-Batch: torch.Size([12000, 28, 28])\n\n\n\nBatches: Khi bạn tải toàn bộ dữ liệu vào mô hình để tính gradient và cập nhật trọng số, chúng ta gọi đó là Batches.\n\nTrong Batches, toàn bộ dữ liệu được sử dụng một lần duy nhất trong mỗi vòng lặp huấn luyện. Trong trường hợp này, batch size bằng với chiều dài của dữ liệu.\n\n\n# Mini Batches \ntrain_batch_size = 64\nvalid_batch_size = 32\n\ntrain_loader = load_data(x_train_norm, y_train, batch_size=train_batch_size, drop_last=True)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=valid_batch_size, test=True)\n\nTotal Mini-Batches: 750\nShape of Each Mini-Batch: torch.Size([64, 28, 28])\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([32, 28, 28])\n\n\n\nMini Batches: Khi bạn tải một phần nhỏ của dữ liệu, với kích thước nằm giữa một mẫu đơn và toàn bộ dữ liệu, chúng ta gọi đó là Mini-Batches.\n\nTrong Mini-Batches, bạn chia dữ liệu thành các nhóm nhỏ có kích thước cố định, gọi là “batch size”. Mỗi batch trong đó chứa một số lượng mẫu dữ liệu được sử dụng để tính gradient và cập nhật trọng số.\n\n\n# SGD (batch_size = 1)\ntrain_batch_size = 1\nvalid_batch_size = 1\n\ntrain_loader = load_data(x_train_norm, y_train, batch_size=train_batch_size)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=valid_batch_size, test=True)\n\nTotal Mini-Batches: 48000\nShape of Each Mini-Batch: torch.Size([1, 28, 28])\n\nTotal Mini-Batches: 12000\nShape of Each Mini-Batch: torch.Size([1, 28, 28])\n\n\n\nSGD: là một phiên bản đặc biệt của mini-batches với batch size bằng 1, nghĩa là mỗi mẫu dữ liệu đều được sử dụng để tính gradient và cập nhật trọng số mô hình riêng lẻ.\n\n# Mini Batches \ntrain_batch_size = 128\nvalid_batch_size = 128\n\ntrain_loader = load_data(x_train_norm, y_train, batch_size=train_batch_size, drop_last=True)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=valid_batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\n\n\nLựa chọn phương pháp\nĐối với dự án này, và thực tế là hầu hết các dự án khác, việc sử dụng Mini-Batches là phương pháp phổ biến và khả thi. Lý do là Mini-Batches cho phép tận dụng sức mạnh của GPU và giúp tiết kiệm đáng kể thời gian huấn luyện mô hình. Do đó, quá trình load data cũng trở thành một phần quan trọng và không thể thiếu trong quá trình xây dựng mô hình Deep Learning.\nTrong tập dữ liệu huấn luyện (train set), việc trộn (shuffle) dữ liệu là bước không thể thiếu để mô hình học một cách tổng quan hơn, như việc khuấy đều mọi thứ trước khi nấu một món ngon. Trong phương pháp Mini-Batches, việc quyết định có nên loại bỏ dữ liệu cuối cùng (drop_last) hay không phụ thuộc vào số lượng mẫu dữ liệu có sẵn trong dự án của bạn. Với dự án này có 60000 mẫu dữ liệu, việc loại bỏ một lượng nhỏ ở mini-batches cuối cùng không ảnh hưởng đáng kể đến sự hiệu quả của mô hình.\nTuy nhiên, đối với tập dữ liệu kiểm tra, bạn không cần phải lo lắng về việc trộn dữ liệu hoặc suy nghĩ về việc có bỏ dữ liệu ở mini-batches cuối cùng hay không. Đơn giản là bạn cứ tải nó vào các Mini-Batches để tối ưu hóa tính toán cho mô hình, như việc thưởng thức một bữa ăn ngon mà không cần phải nấu lại từ đầu.\n\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n    \n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\n\n\nGiờ đây, quá trình tải dữ liệu (load data) trở nên cực kỳ đơn giản. Chỉ cần đưa x và y vào TensorDataset, sau đó sử dụng DataLoader với một số tùy chỉnh cho tập dữ liệu huấn luyện và kiểm tra, và bạn đã có mọi thứ cần thiết cho việc load data. Thật tuyệt vời!” 😄"
  },
  {
    "objectID": "process.html#altogether",
    "href": "process.html#altogether",
    "title": "Process Data",
    "section": "Altogether",
    "text": "Altogether\n\n# Download Data\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# Check the lengths of the datasets\nprint(\"Number of samples in the training set:\", len(train_dataset))\nprint(\"Number of samples in the test set:\", len(test_dataset))\nprint(\"\")\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\n\nx = train_dataset.data\ny = train_dataset.targets\n\nprint(f\"Shape: {x.shape}\")\nprint(f\"Min: {x.min().item()}\")\nprint(f\"Max: {x.max().item()}\")\nprint(f\"Labels: {y.unique()}\")\nprint(\"\")\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\nprint(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\nval_labels, counts = np.unique(y_valid, return_counts=True)\nprint(\"Checking Valid Data Balance:\")\nfor val_label, count in zip(val_labels, counts):\n    print(f\" Label: {val_label}, Count: {count}\")\n\n    \n# Normalization \ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n    \n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n    \n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n    \n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\nprint(\"\")\nprint(f\"Train Data: {x_train_norm.shape}, Min: {x_train_norm.min()}, Max: {x_train_norm.max()}\")\nprint(f\"Valid Data: {x_valid_norm.shape}, Min: {x_valid_norm.min()}, Max: {x_valid_norm.max()}\")\nprint(\"\")\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n    \n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n    \n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nNumber of samples in the training set: 60000\nNumber of samples in the test set: 10000\n\nShape: torch.Size([60000, 28, 28])\nMin: 0\nMax: 255\nLabels: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\ntorch.Size([48000, 28, 28]) torch.Size([48000]) torch.Size([12000, 28, 28]) torch.Size([12000])\nChecking Valid Data Balance:\n Label: 0, Count: 1186\n Label: 1, Count: 1200\n Label: 2, Count: 1187\n Label: 3, Count: 1146\n Label: 4, Count: 1226\n Label: 5, Count: 1203\n Label: 6, Count: 1204\n Label: 7, Count: 1179\n Label: 8, Count: 1262\n Label: 9, Count: 1207\n\nTrain Data: torch.Size([48000, 28, 28]), Min: 0.0, Max: 1.0\nValid Data: torch.Size([12000, 28, 28]), Min: 0.0, Max: 1.0\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 28, 28])"
  },
  {
    "objectID": "neuralnet.html",
    "href": "neuralnet.html",
    "title": "Neural Network",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n### New code\n# Reshape and cast the input data\nx = x.view(-1, 784)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n###\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 784])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 784])\n\n\n\nTrong đoạn mã trên, tôi đã bổ sung một đoạn code đã được dùng nhiều lần ở chương trước để tiến hành việc thay đổi kích thước của biến x, đồng thời thay đổi kiểu dữ liệu (dtype) cho cả x và y. Những thay đổi này được thêm vào để tránh việc lặp lại các bước này khi đang huấn luyện mô hình.\nTrong mạng neural (neural network), có hai khái niệm quan trọng cần được nhắc đến đó là:\n\nHidden Layer (Fully Connected Layer)\nActivation Function"
  },
  {
    "objectID": "neuralnet.html#previous-chapter-code",
    "href": "neuralnet.html#previous-chapter-code",
    "title": "Neural Network",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n### New code\n# Reshape and cast the input data\nx = x.view(-1, 784)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n###\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 784])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 784])\n\n\n\nTrong đoạn mã trên, tôi đã bổ sung một đoạn code đã được dùng nhiều lần ở chương trước để tiến hành việc thay đổi kích thước của biến x, đồng thời thay đổi kiểu dữ liệu (dtype) cho cả x và y. Những thay đổi này được thêm vào để tránh việc lặp lại các bước này khi đang huấn luyện mô hình.\nTrong mạng neural (neural network), có hai khái niệm quan trọng cần được nhắc đến đó là:\n\nHidden Layer (Fully Connected Layer)\nActivation Function"
  },
  {
    "objectID": "neuralnet.html#hidden-layer",
    "href": "neuralnet.html#hidden-layer",
    "title": "Neural Network",
    "section": "Hidden Layer",
    "text": "Hidden Layer\n\nFully connected Layer\n\n\nĐây là hình ảnh của một mạng neural đơn giản, hay có thể gọi là Linear Layer, mà chúng ta đã thảo luận trong chương trước. Trong hình ảnh này, mạng neural đơn giản được minh họa với 5 đầu vào và 3 đầu ra. Tuy nhiên, trong trường hợp dữ liệu Fashion MNIST mà chúng ta đang xem xét, thực tế là có 784 đầu vào và 10 đầu ra. Đây chỉ là một ví dụ minh họa để giúp chúng ta hiểu cách mạng neural hoạt động.\n\nMạng Nơ-ron bao gồm các lớp ẩn (hidden layer) nằm giữa lớp đầu vào và lớp đầu ra. Trong hình ảnh này, có 3 lớp ẩn, mỗi lớp ẩn bao gồm 5 units (biểu tượng bởi các hình tròn trong hình). Trong khi đó, Linear Layer mà chúng ta đã thảo luận trong chương trước được xem như không có lớp ẩn và không có đơn vị (units) nằm ở các lớp ẩn.\nỞ đây, khi một unit được kết nối với tất cả các units ở lớp trước đó, chúng ta có thuật ngữ là Fully Connected Layer (Lớp Kết Nối Đầy Đủ).\nTrong thực tế, quyết định có bao nhiêu lớp hidden layer và mỗi lớp ẩn có bao nhiêu đơn vị là hoàn toàn linh hoạt và phụ thuộc vào bài toán cụ thể bạn đang giải quyết. Điều quan trọng là bạn cần xác định rõ lớp đầu vào và lớp đầu ra. Ví dụ, trong Fashion MNIST, lớp đầu vào luôn có 784 đơn vị và lớp đầu ra luôn có 10 đơn vị để phù hợp với dữ liệu."
  },
  {
    "objectID": "neuralnet.html#activation-function",
    "href": "neuralnet.html#activation-function",
    "title": "Neural Network",
    "section": "Activation Function",
    "text": "Activation Function\n\nHàm kích hoạt (activation function) trong mạng neural là một yếu tố quan trọng quyết định cách mỗi neuron hoặc nút (node) trong mạng phản ứng và truyền giá trị đến các neuron trong lớp tiếp theo. Hàm kích hoạt giúp mạng neural học và biểu diễn các mô hình phức tạp, đặc biệt là các mô hình phi tuyến tính.\nTrong mạng neural, hầu hết các unit hay nút (node) ở các lớp ẩn (hidden layer) đều cần phải đi qua hàm activation, trong khi các unit ở lớp đầu vào (input layer) và lớp đầu ra (output layer) thường không cần sử dụng hàm activation.\nCó một số hàm activation function phổ biến được sử dụng trong deep learning, bao gồm:\n\nSigmoid\nReLu (Rectified Linear Unit)\nSoftmax\n\n\nimport torch\n\nexample = torch.tensor([4., -100., -3., 100.])\n\nsigmoid_result = torch.sigmoid(example)\nsigmoid_result\n\ntensor([0.9820, 0.0000, 0.0474, 1.0000])\n\n\nSigmoid: Hàm kích hoạt này là một hàm đơn giản biến đổi mọi số thành giá trị nằm trong khoảng từ 0 đến 1.\n\nexample = torch.tensor([4., -100., -3., 100.])\n\nrelu_result = torch.relu(example)\nrelu_result\n\ntensor([  4.,   0.,   0., 100.])\n\n\nReLU (Rectified Linear Unit) là một hàm đơn giản với công thức f(x) = max(0, x), nghĩa là nó chuyển đổi mọi giá trị âm thành 0 và giữ nguyên giá trị dương.\n\nexample = torch.tensor([4., -100., -3., 100.])\n\nsoftmax_result = torch.softmax(example, dim=0)\n\n# Print the values of the tensor with decimal formatting\nprint(softmax_result)\nfor value in softmax_result:\n    print(f'{value:.2f}')\n\ntensor([2.0305e-42, 0.0000e+00, 1.4013e-45, 1.0000e+00])\n0.00\n0.00\n0.00\n1.00\n\n\nSoftmax là một hàm kích hoạt (activation function) được sử dụng trong mạng neural để biến đổi một tensor thành một phân phối xác suất, tức là nó chuyển đổi mọi giá trị trong tensor sao cho tổng của chúng bằng 1.\nThông thường, ReLU là một hàm activation function phổ biến được sử dụng trong các lớp ẩn (hidden layers) của mạng neural. Vì vậy, trong mô hình của mình, tôi sẽ lựa chọn sử dụng ReLU như một hàm kích hoạt cho các lớp ẩn của mạng neural."
  },
  {
    "objectID": "neuralnet.html#altogether",
    "href": "neuralnet.html#altogether",
    "title": "Neural Network",
    "section": "Altogether",
    "text": "Altogether\n\ndef train_model(train_loader, valid_loader, loss_fn, \\\n                optimizer_algorithm, lr, n_epochs):\n    \n    # Assume 3 hidden layers, each with 200 units\n    weight_input = torch.randn(784, 200, requires_grad=True)\n    bias_input = torch.zeros(200, requires_grad=True)\n    \n    weight_hd1 = torch.randn(200, 200, requires_grad=True)\n    bias_hd1 = torch.zeros(200, requires_grad=True)\n    \n    weight_hd2 = torch.randn(200, 200, requires_grad=True)\n    bias_hd2 = torch.zeros(200, requires_grad=True)\n    \n    weight_hd3 = torch.randn(200, 10, requires_grad=True)\n    bias_hd3 = torch.zeros(10, requires_grad=True)\n    \n    parameters = [weight_input, bias_input, weight_hd1, bias_hd1, \\\n                  weight_hd2, bias_hd2, weight_hd3, bias_hd3]\n    \n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn(parameters, lr=lr)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n            \n            # Forward pass\n            hd1 = x @ weight_input + bias_input\n            hd1 = torch.relu(hd1)\n            \n            hd2 = hd1 @ weight_hd1 + bias_hd1\n            hd2 = torch.relu(hd2)\n            \n            hd3 = hd2 @ weight_hd2 + bias_hd2\n            hd3 = torch.relu(hd3)\n            \n            out = hd3 @ weight_hd3 + bias_hd3\n            \n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    hd1 = x @ weight_input + bias_input\n                    hd1 = torch.relu(hd1)\n\n                    hd2 = hd1 @ weight_hd1 + bias_hd1\n                    hd2 = torch.relu(hd2)\n\n                    hd3 = hd2 @ weight_hd2 + bias_hd2\n                    hd3 = torch.relu(hd3)\n\n                    out = hd3 @ weight_hd3 + bias_hd3\n                    \n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return parameters, losses, train_acc, valid_acc\n\nTrong đoạn code trên, tôi sử dụng một kiến trúc mạng neural (neural network architecture) với 3 lớp ẩn (hidden layer), mỗi lớp ẩn bao gồm 200 units.\n\nimport torch\nimport torch.nn as nn\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Assume 3 hidden layers, each with 200 units\n        self.input = nn.Linear(784, 200)\n        self.hd1 = nn.Linear(200, 200)\n        self.hd2 = nn.Linear(200, 200)\n        self.hd3 = nn.Linear(200, 10)\n        \n        self.layers = [self.input, self.hd1, self.hd2]\n\n    def forward(self, x):\n        # Forward pass\n        for layer in self.layers:\n            x = layer(x)\n            # Apply ReLU Activation Function\n            x = torch.relu(x)\n            \n        out = self.hd3(x)\n        \n        return out\n\ndef train_model(model, train_loader, valid_loader, \\\n                loss_fn, optimizer_algorithm, lr, n_epochs):\n    \n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn(model.parameters(), lr=lr)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n            out = model.forward(x)\n            \n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    \n                    out = model.forward(x)\n                    \n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\nPhiên bản code trước đó có vẻ “rối như mạng nhện”, tuy nhiên, bây giờ chúng ta đã sử dụng PyTorch như một cây chổi để dọn dẹp cái mạng nhện ấy, để lại cho chúng ta một phiên bản code “sáng sủa như bình minh”.\nBằng cách sử dụng PyTorch, chúng ta đã tạo ra một phiên bản code sạch sẽ và hiệu quả hơn. Điều quan trọng là chúng ta hiểu rõ từng dòng code đang làm gì. Mã clean và PyTorch giống như cặp đôi hoàn hảo, giúp chúng ta tiết kiệm thời gian và nỗ lực trong việc xây dựng các mô hình Deep Learning phức tạp.\n\nimport time\n# Get the start time\nstart_time = time.time()\n\nmodel = Model()\n\n# Define the loss function (CrossEntropyLoss) and optimizer algorithm (Adam)\nloss_fn = nn.CrossEntropyLoss()\noptimizer_algorithm = \"Adam\"\n\n# Set the learning rate and number of training epochs\nlearning_rate = 0.01\nn_epochs = 5\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             loss_fn, \\\n                                                             optimizer_algorithm, \\\n                                                             learning_rate, \\\n                                                             n_epochs)\n\n# Get the end time\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\n\n# Convert to minutes and seconds\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2) \n\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n\nModel training time: 11 min 41.77s\n\n\n\nimport matplotlib.pyplot as plt\n\nfinal_loss = losses.mean(1)[-1]\nfinal_train_acc = train_acc.mean(1)[-1]\nfinal_valid_acc = valid_acc.mean(1)[-1]\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 3))\n\naxs[0].plot(range(losses.shape[0]), losses.mean(1), \"-o\")\naxs[0].set_title(f\"Train Loss is: {final_loss:.4f}\")\n\n\naxs[1].plot(range(train_acc.shape[0]), train_acc.mean(1), \"-o\")\naxs[1].plot(range(valid_acc.shape[0]), valid_acc.mean(1), \"-o\")\naxs[1].set_title(f\"Train: {final_train_acc:.2f}%, Valid: {final_valid_acc:.2f}%\")\naxs[1].legend([\"Train\", \"Valid\"])\n\nplt.suptitle(f\"Result with Neural Network\", fontsize = 16)\nplt.show()\n\n\n\n\nKết quả cao hơn so với simple neural network ở chương trước, xác suất dự đoán cao trên 85%. Bây giờ, hãy cùng kiểm tra để xem liệu kết quả trên tập dữ liệu kiểm tra (test set) có cao hơn so với simple neural network không.\n\nx_test = test_dataset.data\ny_test = test_dataset.targets\n\n# Reshape and cast the input data\nx_test = x_test.view(-1, 784)\nx_test = x_test.to(torch.float32)\nx_test_norm = min_max_scale(x_test)\n\n# Cast the target labels to the 'long' data type\ny_test = y_test.to(torch.long)\n\n\nout = model.forward(x_test_norm)\n\ntest_acc = 100 * (out.argmax(1) == y_test).float().mean()\n\nids_error = (out.argmax(1) != y_test).nonzero()\nprint(f\"In {len(y_test)} images, model guess wrong {len(ids_error)}\")\nprint(f\"Accuracy is: {test_acc:.2f}%\")\n\nIn 10000 images, model guess wrong 1458\nAccuracy is: 85.42%\n\n\nVới mô hình Neural Network và sau 5 epochs, chúng ta đã đạt được độ chính xác cao hơn 85% trên tập dữ liệu kiểm tra. Đây là một bước tiến so với trên 80% của Linear Layer mà chúng ta đã đạt được trong chương trước. Điều này chỉ ra rằng Neural Network có tiềm năng mạnh mẽ hơn đối với bài toán này.\nTuy nhiên, câu hỏi đặt ra là liệu chúng ta có thể làm cho mô hình tốt hơn nữa không? Có phương pháp nào để tối ưu hóa mô hình hiện tại và nâng cao xác suất dự đoán?\nChúng ta sẽ tiếp tục khám phá những phương pháp tiên tiến trong các chương tiếp theo, bao gồm Regularization để ngăn chặn overfitting, kết hợp với sử dụng GPU để tăng tốc độ huấn luyện và cải thiện hiệu suất. Đó chính là những cách mạnh mẽ để cải thiện hiệu suất của mô hình và làm cho nó trở nên mạnh mẽ và ổn định hơn!\n\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\n\nfor ax in axs.flatten():\n    # Select a random index from the output\n    random_index = np.random.choice(len(out))\n    \n    # Display the image at the selected index with colormap gray for correct predictions\n    true_label = y_test[random_index]\n    guess_label = out.argmax(1)[random_index]\n    \n    if true_label == guess_label:\n        ax.imshow(x_test[random_index].view(28, 28), cmap=\"gray\")\n    else:\n        ax.imshow(x_test[random_index].view(28, 28))\n    \n    \n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Result\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Project Fashion MNIST",
    "section": "",
    "text": "Mục tiêu của tôi trong dự án này là giúp bản thân của mình, sau hai năm xa rời thế giới Deep Learning và AI, sau khi đọc lại toàn bộ dự án này, có thể nhớ và nắm lại được một cái nhìn tổng quan về quá trình làm việc với Deep Learning.\nChỉ còn ba tháng nữa, tôi sẽ bước vào cuộc sống quân đội, nơi mà Deep Learning và AI sẽ trở nên xa lạ. Điều này giống như một câu chuyện về người đam mê bóng đá bị chấn thương đầu gối và phải tạm biệt sân cỏ trong hai năm dài. Nhưng đam mê đó vẫn nhen nhóm bên trong, và họ muốn duy trì nó. Thay vì phải bắt đầu lại từ đầu, học lại từng kỹ thuật nhỏ nhặt như cách sút bóng hoặc chạy chỗ, họ muốn tham gia vào một trận đấu bóng vui vẻ với bạn bè để cùng nhau tái hiện “bức tranh tổng thể” của cuộc hành trình đá bóng. Điều này sẽ giúp họ hiểu rõ những điểm quan trọng nhất trong quá trình đá bóng. Sau đó, họ sẽ có thể dễ dàng học lại các kỹ thuật cụ thể để nâng cao trình độ đá bóng của mình và duy trì đam mê đối với môn thể thao yêu thích.\nDự án này cũng tương tự, mục tiêu của nó là giúp tôi sau hai năm có khả năng hiểu và nắm vững một cái nhìn tổng quan về quá trình làm việc với Deep Learning. Sau đó, tôi sẽ chia sẻ những tài liệu, kiến thức sâu hơn để tìm hiểu về các khái niệm cốt lõi trong Deep Learning. Mục tiêu cuối cùng là duy trì và kích thích niềm đam mê của tôi, để tôi có thể tiếp tục khám phá và đi sâu vào thế giới hấp dẫn của Deep Learning và AI."
  },
  {
    "objectID": "index.html#the-goal",
    "href": "index.html#the-goal",
    "title": "Project Fashion MNIST",
    "section": "",
    "text": "Mục tiêu của tôi trong dự án này là giúp bản thân của mình, sau hai năm xa rời thế giới Deep Learning và AI, sau khi đọc lại toàn bộ dự án này, có thể nhớ và nắm lại được một cái nhìn tổng quan về quá trình làm việc với Deep Learning.\nChỉ còn ba tháng nữa, tôi sẽ bước vào cuộc sống quân đội, nơi mà Deep Learning và AI sẽ trở nên xa lạ. Điều này giống như một câu chuyện về người đam mê bóng đá bị chấn thương đầu gối và phải tạm biệt sân cỏ trong hai năm dài. Nhưng đam mê đó vẫn nhen nhóm bên trong, và họ muốn duy trì nó. Thay vì phải bắt đầu lại từ đầu, học lại từng kỹ thuật nhỏ nhặt như cách sút bóng hoặc chạy chỗ, họ muốn tham gia vào một trận đấu bóng vui vẻ với bạn bè để cùng nhau tái hiện “bức tranh tổng thể” của cuộc hành trình đá bóng. Điều này sẽ giúp họ hiểu rõ những điểm quan trọng nhất trong quá trình đá bóng. Sau đó, họ sẽ có thể dễ dàng học lại các kỹ thuật cụ thể để nâng cao trình độ đá bóng của mình và duy trì đam mê đối với môn thể thao yêu thích.\nDự án này cũng tương tự, mục tiêu của nó là giúp tôi sau hai năm có khả năng hiểu và nắm vững một cái nhìn tổng quan về quá trình làm việc với Deep Learning. Sau đó, tôi sẽ chia sẻ những tài liệu, kiến thức sâu hơn để tìm hiểu về các khái niệm cốt lõi trong Deep Learning. Mục tiêu cuối cùng là duy trì và kích thích niềm đam mê của tôi, để tôi có thể tiếp tục khám phá và đi sâu vào thế giới hấp dẫn của Deep Learning và AI."
  },
  {
    "objectID": "index.html#why-fashion-mnist",
    "href": "index.html#why-fashion-mnist",
    "title": "Project Fashion MNIST",
    "section": "Why Fashion MNIST?",
    "text": "Why Fashion MNIST?\nTôi đã lựa chọn bộ dữ liệu Fashion MNIST để thực hiện mục tiêu của mình vì đây là một sự kết hợp tuyệt vời giữa đơn giản và thú vị. Những hình ảnh trong bộ dữ liệu này sẽ khơi gợi sự tò mò và đam mê của tôi khi quay trở lại thế giới của Deep Learning.\nNhưng điều thú vị ở đây là sự đơn giản của Fashion MNIST không chỉ làm cho dự án dễ tiếp cận mà còn giúp tôi tập trung vào mục tiêu chính: hiểu rõ và nắm bắt được cái nhìn tổng quan về Deep Learning. Không bị lạc hướng bởi sự phức tạp, tôi có thể dễ dàng tiến đến mục tiêu của mình - có lại được một bức tranh toàn cảnh trong lĩnh vực này."
  },
  {
    "objectID": "index.html#writing-style",
    "href": "index.html#writing-style",
    "title": "Project Fashion MNIST",
    "section": "Writing Style",
    "text": "Writing Style\nTrong dự án này, tôi có xu hướng viết mã trước và sau đó giải thích những gì tôi đã thực hiện. Lý do là dự án này không phải là để đọc, vì vậy tôi muốn tránh viết theo kiểu thông thường, có thể khiến bạn đọc chỉ đọc qua mà không tập trung vào mã nguồn. Thay vào đó, mục tiêu của tôi là tạo ra bài viết hướng đến mục đích cụ thể mà tôi muốn đạt được.\nMột điểm quan trọng khác là bạn có thể thấy các tiêu đề, đề mục và mục lục tôi đều viết bằng tiếng Anh. Đúng vậy, điều này không phải là ngẫu nhiên. Mục tiêu của dự án này không chỉ dừng lại ở đây, mà là để giúp bạn đọc phát triển và tiếp tục học hỏi. Khi bạn hoàn thành dự án này, đó chỉ là một bước khởi đầu. Lĩnh vực AI, Deep Learning luôn thay đổi rất nhanh, và tất cả thông tin mới nhất đều xuất phát từ tiếng Anh. Do đó, tôi muốn khuyến khích bạn đọc làm quen với tiếng Anh, vì tất cả bài báo và tài liệu về khoa học máy tính đều được viết bằng tiếng Anh. Hiện tại, chưa có đủ nguồn tài liệu chất lượng được dịch sang tiếng Việt, và ngay cả khi có, thì việc dịch và cập nhật sẽ tốn rất nhiều thời gian."
  },
  {
    "objectID": "linearlayer.html",
    "href": "linearlayer.html",
    "title": "Linear Layer",
    "section": "",
    "text": "# Download Data\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization \ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n    \n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n    \n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n    \n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n    \n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n    \n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\n\n\nTrong đoạn mã trên, tôi chỉ loại bỏ các câu lệnh print không cần thiết và giữ nguyên phần phần còn lại của đoạn code như trong chương trước đó."
  },
  {
    "objectID": "linearlayer.html#previous-chapter-code",
    "href": "linearlayer.html#previous-chapter-code",
    "title": "Linear Layer",
    "section": "",
    "text": "# Download Data\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization \ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n    \n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n    \n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n    \n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n    \n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n    \n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 28, 28])\n\n\n\nTrong đoạn mã trên, tôi chỉ loại bỏ các câu lệnh print không cần thiết và giữ nguyên phần phần còn lại của đoạn code như trong chương trước đó."
  },
  {
    "objectID": "linearlayer.html#forward-propagation",
    "href": "linearlayer.html#forward-propagation",
    "title": "Linear Layer",
    "section": "Forward Propagation",
    "text": "Forward Propagation\n\nimport torch\n\n# Get the first min batch of data from the train_loader\nsample_x, sample_y = next(iter(train_loader))\n\n# Reshape X to match the neural network model input size (flatten)\nsample_x = sample_x.view(-1, 784)\n\n# Convert X to the appropriate data type (float32)\nsample_x = sample_x.to(torch.float32)\n\n# Define the number of output classes\nn_out = 10\n\n# Initialize weight and bias tensors with requires_grad=True\nweight = torch.randn(784, n_out, requires_grad = True)\nbias = torch.zeros(n_out, requires_grad = True)\n\n# Perform the forward pass through the neural network\nsample_out = sample_x @ weight + bias\n\n# Check the shape of the output\nsample_out.shape\n\ntorch.Size([128, 10])\n\n\nLinear Layer: Một trong những Mô hình Cơ bản của Deep Learning\nLinear Layer (Lớp Tuyến tính) là một trong những mô hình cơ bản nhất trong lĩnh vực Deep Learning. Mô hình này chỉ cần một đầu vào đã được làm phẳng (flatten), một trọng số (weight) và một sai số (bias) để hoạt động. Các thành phần này hoạt động cùng nhau để tạo ra dự đoán cho biến mục tiêu của dự án.\nCông thức Linear Layer:\nDự đoán (Prediction) được tính bằng công thức sau:\n\\[ \\text{Prediction} = (\\text{Input} \\cdot \\text{Weight}) + \\text{Bias} \\]\nTrong đó:\n\nPrediction: Kết quả đầu ra mong muốn.\nInput: là vector đầu vào đã được làm phẳng.\nWeight: là trọng số cần tìm để tối ưu hóa mô hình.\nBias: là giá trị điều chỉnh cho dự đoán.\n\nTrong thực tế, trọng số (weight) thường được khởi tạo ngẫu nhiên theo phân phối đồng nhất (uniform distribution), và Bias thường bắt đầu từ giá trị 0.\n\nimport matplotlib.pyplot as plt\n\n# Create subplots with 2 rows and 4 columns\nfig, axs = plt.subplots(3, 3, figsize=(14, 8))\n\n# Iterate through each subplot\nfor ax in axs.flatten():\n    # Select a random index from the batch\n    random_index = np.random.choice(len(sample_out))\n\n    # Display the image at the selected index\n    ax.imshow(sample_x[random_index].view(28, 28))\n    \n    # Get the true label and predicted label for the selected index\n    true_label = sample_y[random_index]\n    guess_label = sample_out.argmax(1)[random_index]\n    \n    # Set the title of the subplot with the true and predicted labels\n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    \n    # Turn off axis for better visualization\n    ax.axis(\"off\")\n\n# Add a title to the entire figure\nplt.suptitle(\"Forward Propagation\", fontsize=16)\n\n# Show the plot\nplt.show()\n\n\n\n\nForward Propagation, giống như tên của nó, là quá trình diễn ra từ đầu vào đến đầu ra mục tiêu mà không quan tâm đến kết quả cuối cùng.\nĐể hình dung, hãy nghĩ về việc bạn đang làm một bài thi. Trong quá trình bạn làm xong bài đó tới khi hết giờ, bạn không cần biết điểm cuối cùng của mình sẽ là bao nhiêu. Forward Propagation chính là giai đoạn này trong Deep Learning, nó là bước đầu tiên và quan trọng nhất trong việc tính toán dự đoán của mô hình trước khi xem xét các bước sau như tính toán loss và cập nhật trọng số trong quá trình huấn luyện."
  },
  {
    "objectID": "linearlayer.html#loss-function",
    "href": "linearlayer.html#loss-function",
    "title": "Linear Layer",
    "section": "Loss Function",
    "text": "Loss Function\nTrong Deep Learning, có 3 hàm Loss Function rất phổ biến và thường được sử dụng trong hầu hết các dự án:\n\nBinary Cross-Entropy (BCE) Loss\nMean Squared Error (MSE) Loss\nCross-Entropy Loss\n\nSự lựa chọn giữa các hàm Loss Function này phụ thuộc vào mục tiêu cụ thể của mỗi dự án.\n\nimport torch.nn as nn\nloss_fn_bce = nn.BCELoss()\n\nBinary Cross-Entropy (BCE) Loss: Thường được sử dụng trong các nhiệm vụ phân loại nhị phân. BCE Loss giúp đo lường mức độ sai lệch giữa dự đoán và thực tế khi chỉ có hai lớp đối tượng.\nĐể hình dung, hãy tưởng tượng hôm nay là những ngày cuối năm học, bạn đã thu thập tất cả các điểm thi của mình (là đầu vào), và bạn muốn sử dụng những điểm số này để dự đoán xem liệu bạn có đủ điểm để được xếp vào danh sách “Học sinh Giỏi” hay “không”. Trong tình huống này, bạn sẽ nên sử dụng Binary Cross-Entropy Loss để đo lường sự chênh lệch giữa dự đoán của bạn và thực tế. BCE Loss giúp bạn đánh giá mức độ “đúng” của dự đoán dựa trên hai khả năng: “có” hoặc “không có” danh hiệu Học sinh Giỏi.\n\nimport torch.nn as nn\nloss_fn_ce = nn.CrossEntropyLoss()\n\nCross-Entropy Loss: Thường được sử dụng trong các tác vụ phân loại đa lớp. CE Loss giúp đo độ khác biệt giữa phân phối xác suất của dự đoán và phân phối xác suất thực tế của các lớp.\nĐể hình dung, (giống như ví dụ trước) hôm nay là những ngày cuối năm học và đã thu thập tất cả điểm số từ các kỳ thi của mình (đây là dữ liệu đầu vào). Thay vì bạn chỉ quan tâm đến việc bạn có đủ điểm để được xếp hạng Giỏi hay không, bạn muốn biết chính xác bạn thuộc loại Giỏi, Khá, Trung bình. Trong tình huống này, bạn sẽ nên sử dụng Cross-Entropy Loss để đánh giá mức độ khác biệt giữa dự đoán của bạn và thực tế. CE Loss giúp bạn đo đạc mức độ “đúng” của dự đoán dựa trên khả năng phân loại chính xác vào từng loại danh hiệu học tập khác nhau, chẳng hạn như Giỏi, Khá, và Trung bình.\n\nimport torch.nn as nn\nloss_fn_mse = nn.MSELoss()\n\nMean Squared Error (MSE) Loss: Thường được ưa chuộng trong các tác vụ dự đoán và hồi quy. MSE Loss đo độ lớn của sai số bình phương giữa dự đoán và giá trị thực tế. Ví dụ như là dự đoán giá nhà, dự đoán giá sản phẩm.\nĐể hình dung, (giống như ví dụ trước) hôm nay là những ngày cuối năm học và đã thu thập tất cả điểm số từ các kỳ thi của mình (đây là dữ liệu đầu vào). Thay vì chỉ quan tâm đến việc bạn có đủ điểm để được xếp hạng Giỏi, Khá, hay Trung bình, bạn muốn biết chính xác điểm tổng kết năm học của bạn sẽ là bao nhiêu. Trong tình huống này, bạn sẽ nên sử dụng Mean Squared Error (MSE) Loss để đánh giá mức độ khác biệt giữa dự đoán của bạn và thực tế. MSE Loss giúp bạn đo đạc mức độ sai lệch bình phương giữa dự đoán và điểm tổng kết thực tế, cho phép bạn biết được mức độ chính xác của dự đoán về điểm số.\n\nloss_fn = nn.CrossEntropyLoss()\n\n# Converting the data type of sample_y to Long.\nsample_y = sample_y.to(torch.long)\n\nsample_loss = loss_fn(sample_out, sample_y)\nprint(f\"Loss: {sample_loss}\")\nsample_loss\n\nLoss: 15.463730812072754\n\n\ntensor(15.4637, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nTrong dự án Fashion MNIST này, với mục tiêu là phân loại thành 10 nhãn, việc sử dụng Cross-Entropy Loss là lựa chọn phù hợp nhất.\nHàm CrossEntropyLoss trong PyTorch đòi hỏi đầu vào y_true (targets) phải có kiểu dữ liệu là Long (integer) và chứa các nhãn thực tế, cùng với dự đoán (output prediction) của mô hình."
  },
  {
    "objectID": "linearlayer.html#optimization-algorithms",
    "href": "linearlayer.html#optimization-algorithms",
    "title": "Linear Layer",
    "section": "Optimization Algorithms",
    "text": "Optimization Algorithms\nCó 3 thuật toántối ưu hóa phổ biến trong giảng dạy, thử nghiệm và cải thiện mô hình:\n\nSGD: Stochastic Gradient Descent\nAdam: Adaptive Moment Estimation\nRMSProp: Root Mean Square Propagation\n\n\nparameters = [weight, bias]\nlearning_rate = .01\n\n# SGD\noptimizer_sgd = torch.optim.SGD(parameters, lr = learning_rate)\n\nSGD (Stochastic Gradient Descent) có thể coi là một trong những thuật toán tối ưu hóa (optimizer) cơ bản nhất trong Deep Learning. Nó tương tự như Linear Layer, được xem như một trong những nền tảng cơ bản nhất để phát triển mô hình Neural Network trong Deep Learning. SGD cung cấp một nền tảng quan trọng để từ đó các thuật toán tối ưu hóa nâng cao như Adam và RMSProp được phát triển và hoạt động\n\nparameters = [weight, bias]\nlearning_rate = .01\n\n# RMSProp \noptimizer_rms = torch.optim.RMSprop(parameters, lr = learning_rate) \n\n# Adam\noptimizer_adam = torch.optim.Adam(parameters, lr = learning_rate)\n\nAdam và RMSProp có thể được xem là những thuật toán tối ưu hóa (optimizer) tiến bộ hơn của SGD trong Deep Learning. Chúng thường giúp cho mô hình học nhanh và hiệu quả hơn.\nĐể dễ hình dung, hãy nghĩ đến một tình huống trong đó bạn cần ôn thi với một đề cương dày đặc. Thay vì phải đọc toàn bộ đề cương, điều này có thể mất nhiều thời gian, bạn quyết định sử dụng một phương pháp thông minh hơn. Bạn tập trung vào việc học các từ khóa chính của từng câu hỏi trong đề cương. Bằng cách này, bạn vẫn có thể đạt điểm cao trong kì thi nhưng tiết kiệm được rất nhiều thời gian.\nTương tự, Adam và RMSProp giúp mô hình Deep Learning tập trung vào những điểm quan trọng trong quá trình học, điều này giúp cải thiện hiệu suất huấn luyện và tiết kiệm thời gian so với việc sử dụng SGD truyền thống.\n\nparameters = [weight, bias]\nlearning_rate = .01\n\n# Use Adam\noptimizer = torch.optim.Adam(parameters, lr = learning_rate)\n\nTrước tiên, tôi sẽ tiến hành thử nghiệm mô hình bằng cách sử dụng thuật toán Adam. Sau khi đã tiến đến giai đoạn kiểm tra và có cái nhìn tổng quan về hiệu suất của mô hình, tôi sẽ xem xét việc tối ưu hóa mô hình bằng cách thử nghiệm với RMSProp optimizer."
  },
  {
    "objectID": "linearlayer.html#backward-propagation",
    "href": "linearlayer.html#backward-propagation",
    "title": "Linear Layer",
    "section": "Backward Propagation",
    "text": "Backward Propagation\n\nfor i in range(2):\n    print(f\"Loss Before Backward: {sample_loss}\")\n    \n    # Backward Propagation \n    sample_loss.backward()\n    \n    # Update parameters\n    optimizer.step()\n    optimizer.zero_grad()\n\n    # Forward Propagation\n    sample_out = sample_x @ weight + bias\n    sample_loss = loss_fn(sample_out, sample_y)\n\n    print(f\"Loss After Backward: {sample_loss}\")\n    print(\"\")\n\nLoss Before Backward: 15.463730812072754\nLoss After Backward: 13.028249740600586\n\nLoss Before Backward: 13.028249740600586\nLoss After Backward: 10.919900894165039\n\n\n\nBackpropagation là quá trình quan trọng trong huấn luyện mạng neural để giảm thiểu độ lỗi (loss), tức là để đưa ra các dự đoán (predictions) gần giống với các giá trị thực tế (labels). Nó cho phép mô hình của bạn hiệu quả hơn trong việc dự đoán kết quả dựa trên dữ liệu đầu vào.\nThuật toán chính được sử dụng để điều chỉnh các tham số của mạng neural trong quá trình này gọi là Gradient Descent. Cụ thể, bạn tính toán độ dốc (gradient) của mỗi tham số, sau đó điều chỉnh các tham số đó bằng cách trừ một lượng nhất định (được điều chỉnh bởi learning rate) nhân với độ dốc (gradient). Quá trình này được lặp lại nhiều lần để dần dần cải thiện khả năng dự đoán của mô hình.\nVì vậy, tổng hợp lại, Backpropagation là quá trình tối ưu hóa mô hình thông qua Gradient Descent, giúp mô hình học từ dữ liệu và dự đoán chính xác hơn kết quả mà bạn mong muốn bằng cách điều chỉnh các tham số dựa trên độ dốc (gradient) và Learning Rate của loss function.\n\n# Epoch\nn_epochs = 3\n\nfor epoch in range(n_epochs):\n    # Backward Propagation \n    sample_loss.backward()\n    \n    # Update parameters\n    optimizer.step()\n    optimizer.zero_grad()\n\n    # Forward Propagation\n    sample_out = sample_x @ weight + bias\n    sample_loss = loss_fn(sample_out, sample_y)\n\n    print(f\"Epoch {epoch}: {sample_loss}\")\n\nEpoch 0: 9.452093124389648\nEpoch 1: 8.548746109008789\nEpoch 2: 7.9047627449035645\n\n\nHãy tưởng tượng rằng bạn đang tham gia vào một cuộc thi để cố gắng đạt được điểm số cao nhất mà bạn có thể. Hãy xem xét quy trình sau:\nBước 1 - Forward: Lần đầu tiên khi bạn bắt đầu làm bài thi, bạn chỉ cố gắng làm xong tất cả các phần của bài thi mà không quan tâm đến điểm số cuối cùng. Trong ngữ cảnh của deep learning, điều này tương đương với việc sử dụng dữ liệu đầu vào để tính toán và tạo ra một đầu ra dự đoán.\nBước 2 - Loss: Sau khi bạn đã hoàn thành bài thi, bạn nhận được điểm số của mình từ giáo viên. Lúc này, bạn có thể so sánh điểm số này với điểm số tối đa có thể đạt được trong bài thi đó. Sự khác biệt giữa điểm số của bạn và điểm số tối đa đó được gọi là loss. Trong Deep Learning, Loss là một số đo lường sự sai khác giữa đầu ra dự đoán và đầu ra thực tế mà chúng ta muốn.\nBước 3 - Backward: Với thông tin về loss, bạn bắt đầu nghiên cứu và học từ những câu hỏi hoặc phần của bài thi mà bạn đã trả lời sai hoặc không chắc chắn. Bạn cố gắng hiểu rõ hơn về chúng và cách cải thiện kết quả của mình. Trong deep learning, quá trình này được gọi là backward hoặc backpropagation. Nó giúp điều chỉnh các tham số trong neural network để giảm thiểu loss.\nEpoch (chu kỳ): Mỗi lần bạn thực hiện quá trình từ “Forward” đến “Backward” từ đầu đến cuối, đó là một chu kỳ hoàn chỉnh (1 epoch) trong cuộc thi của bạn, và được gọi là một “Epoch.”\nVậy, trong bối cảnh của deep learning, một “Epoch” là một chu kỳ hoàn chỉnh của quá trình huấn luyện mô hình, bao gồm Forward, tính toán Loss, và Backward để cải thiện mô hình. Việc lặp lại nhiều Epochs giúp mô hình học được từ dữ liệu và cải thiện khả năng dự đoán của nó."
  },
  {
    "objectID": "linearlayer.html#altogether",
    "href": "linearlayer.html#altogether",
    "title": "Linear Layer",
    "section": "Altogether",
    "text": "Altogether\n\nUse Sample Data\n\n# Get a first mini batch of data\nsample_x, sample_y = next(iter(train_loader))\n\n# Reshape and cast the input data\nsample_x = sample_x.view(-1, 784)\nsample_x = sample_x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\nsample_y = sample_y.to(torch.long)\n\nprint(sample_x.shape, sample_y.shape)\n\ntorch.Size([128, 784]) torch.Size([128])\n\n\n\nimport torch\n\ndef train_model(input, targets, loss_fn, optimizer_algorithm, lr, n_epochs):\n    # Initialize weights and bias\n    weight = torch.randn(784, 10, requires_grad=True)\n    bias = torch.zeros(10, requires_grad=True)\n    \n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn([weight, bias], lr=lr)\n    \n    # Lists to store losses and training accuracy\n    losses = []\n    train_acc = []\n    \n    for epoch in range(n_epochs):\n        # Forward pass\n        out = input @ weight + bias\n\n        # Calculate loss\n        loss = loss_fn(out, targets)\n        \n        # Backpropagation\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        \n        # Store the loss\n        losses.append(loss.item())\n    \n        # Calculate training accuracy\n        acc = 100 * (out.argmax(1) == targets).float().mean()\n        train_acc.append(acc)\n        \n    return out, losses, train_acc\n\n\n# Define the loss function (CrossEntropyLoss) and optimizer algorithm (Adam)\nloss_fn = nn.CrossEntropyLoss()\noptimizer_algorithm = \"Adam\"\n\n# Set the learning rate and number of training epochs\nlearning_rate = 0.01\nn_epochs = 30\n\n# Call the 'train_model' function to train the model\nout, losses, train_acc = train_model(sample_x, sample_y, loss_fn, optimizer_algorithm, learning_rate, n_epochs)\n\n# Retrieve the final loss and training accuracy\nfinal_loss = losses[-1]\nfinal_accuracy = train_acc[-1]\n\n# Print or use the final_loss and final_accuracy as needed\nprint(f\"Final Loss: {final_loss:.4f}\")\nprint(f\"Final Training Accuracy: {final_accuracy:.2f}%\")\n\nFinal Loss: 1.8055\nFinal Training Accuracy: 64.84%\n\n\n\nfig, ax = plt.subplots(1, 2, figsize = (14, 3))\n\nax[0].plot(losses)\nax[0].set_title(f\"Epoch {n_epochs}, Loss is: {final_loss:.4f}\")\nax[0].set_xlabel(\"Epochs\")\nax[0].set_ylabel(\"Loss\")\n\nax[1].plot(train_acc)\nax[1].set_title(f\"Epoch {n_epochs}, Accuracy is: {final_accuracy:.2f}%\")\nax[1].set_xlabel(\"Epochs\")\nax[1].set_ylabel(\"Accuracy\")\nplt.show()\n\n\n\n\n\nids_error = (out.argmax(1) != sample_y).nonzero()\nprint(f\"In {len(sample_y)} images, model guess wrong {len(ids_error)}\")\n\nIn 128 images, model guess wrong 45\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\n\nfor ax in axs.flatten():\n    # Select a random index from the output\n    random_index = np.random.choice(len(out))\n    \n    # Display the image at the selected index with colormap gray for correct predictions\n    true_label = sample_y[random_index]\n    guess_label = out.argmax(1)[random_index]\n    \n    if true_label == guess_label:\n        ax.imshow(sample_x[random_index].view(28, 28), cmap=\"gray\")\n    else:\n        ax.imshow(sample_x[random_index].view(28, 28))\n    \n    \n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Result\", fontsize=16)\nplt.show()\n\n\n\n\n\n\nUse All Data\n\nimport torch\n\ndef train_model(train_loader, valid_loader, loss_fn, optimizer_algorithm, lr, n_epochs):\n    # Initialize weights and bias\n    weight = torch.randn(784, 10, requires_grad=True)\n    bias = torch.zeros(10, requires_grad=True)\n    \n    # Get the optimizer function based on the provided algorithm name\n    opt_fn = getattr(torch.optim, optimizer_algorithm)\n    optimizer = opt_fn([weight, bias], lr=lr)\n    \n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n    \n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n            # Reshape and cast the input data\n            x = x.view(-1, 784)\n            x = x.to(torch.float32)\n\n            # Cast the target labels to the 'long' data type\n            y = y.to(torch.long)\n            \n            # Forward pass\n            out = x @ weight + bias\n\n            # Calculate loss\n            loss = loss_fn(out, y)\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y).float().mean()\n            train_acc[epoch, i] = acc\n        \n            # Evaluation in test data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n                    x = x.view(-1, 784)\n                    x = x.to(torch.float32)\n\n                    # Cast the target labels to the 'long' data type\n                    y = y.to(torch.long)\n                \n                    out = x @ weight + bias\n\n                    acc = 100 * (out.argmax(1) == y).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return [weight, bias], losses, train_acc, valid_acc\n\n\nimport time\n\n# Get the start time\nstart_time = time.time()\n\n# Define the loss function (CrossEntropyLoss) and optimizer algorithm (Adam)\nloss_fn = nn.CrossEntropyLoss()\noptimizer_algorithm = \"Adam\"\n\n# Set the learning rate and number of training epochs\nlearning_rate = 0.01\nn_epochs = 5\n\n# Call the 'train_model' function to train the model with all data\nparameters, losses, train_acc, valid_acc = train_model(train_loader, \\\n                                                       valid_loader, \\\n                                                       loss_fn, \\\n                                                       optimizer_algorithm, \\\n                                                       learning_rate, \\\n                                                       n_epochs)\n\n# Get the end time\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\n\n# Convert to minutes and seconds\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2) \n\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n\nModel training time: 8 min 34.2s\n\n\n\nfinal_loss = losses.mean(1)[-1]\nfinal_train_acc = train_acc.mean(1)[-1]\nfinal_valid_acc = valid_acc.mean(1)[-1]\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 3))\n\naxs[0].plot(range(losses.shape[0]), losses.mean(1), \"-o\")\naxs[0].set_title(f\"Train Loss is: {final_loss:.4f}\")\n\n\naxs[1].plot(range(train_acc.shape[0]), train_acc.mean(1), \"-o\")\naxs[1].plot(range(valid_acc.shape[0]), valid_acc.mean(1), \"-o\")\naxs[1].set_title(f\"Train: {final_train_acc:.2f}%, Valid: {final_valid_acc:.2f}%\")\naxs[1].legend([\"Train\", \"Valid\"])\n\nplt.suptitle(f\"Result with Linear Layer\", fontsize = 16)\nplt.show()\n\n\n\n\nKết quả cũng khá cao, hơn 80% trên tập dữ liệu validation bằng mô hình Linear Layer (mô hình neural đơn giản nhất) sau 5 epochs. Bây giờ, hãy cùng kiểm tra để xem liệu chúng ta có thể đạt được bao nhiêu phần trăm độ chính xác trên tập dữ liệu kiểm tra (test set).\n\nx_test = test_dataset.data\ny_test = test_dataset.targets\n\n# Reshape and cast the input data\nx_test = x_test.view(-1, 784)\nx_test = x_test.to(torch.float32)\nx_test_norm = min_max_scale(x_test)\n\n# Cast the target labels to the 'long' data type\ny_test = y_test.to(torch.long)\n\n\nweight = parameters[0]\nbias = parameters[1]\n\nout = x_test_norm @ weight + bias\n\ntest_acc = 100 * (out.argmax(1) == y_test).float().mean()\n\nids_error = (out.argmax(1) != y_test).nonzero()\nprint(f\"In {len(y_test)} images, model guess wrong {len(ids_error)}\")\nprint(f\"Accuracy is: {test_acc:.2f}%\")\n\nIn 10000 images, model guess wrong 1952\nAccuracy is: 80.48%\n\n\nVới mô hình Linear Layer và 5 epochs, chúng ta đã đạt được độ chính xác xấp xỉ 80% trên tập dữ liệu kiểm tra (test set). Giờ hãy thử sử dụng một mạng neural (neural network) để xem liệu chúng ta có thể đạt được độ chính xác cao hơn không! Hãy cùng chờ xem kết quả sẽ thế nào!\n\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\n\nfor ax in axs.flatten():\n    # Select a random index from the output\n    random_index = np.random.choice(len(out))\n    \n    # Display the image at the selected index with colormap gray for correct predictions\n    true_label = y_test[random_index]\n    guess_label = out.argmax(1)[random_index]\n    \n    if true_label == guess_label:\n        ax.imshow(x_test[random_index].view(28, 28), cmap=\"gray\")\n    else:\n        ax.imshow(x_test[random_index].view(28, 28))\n    \n    \n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Result\", fontsize=16)\nplt.show()"
  },
  {
    "objectID": "finetuning.html",
    "href": "finetuning.html",
    "title": "Fine Tuning",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Reshape and cast the input data\nx = x.unsqueeze(1)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 1, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 1, 28, 28])"
  },
  {
    "objectID": "finetuning.html#previouse-chapter-code",
    "href": "finetuning.html#previouse-chapter-code",
    "title": "Fine Tuning",
    "section": "",
    "text": "# Download Data\nimport torch\nimport numpy as np\nfrom torchvision import datasets\n\ntrain_dataset = datasets.FashionMNIST(root='./data', train=True, download=True)\ntest_dataset = datasets.FashionMNIST(root='./data', train=False, download=True)\n\n# constant for classes\nclasses = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n\nx = train_dataset.data\ny = train_dataset.targets\n\n# Reshape and cast the input data\nx = x.unsqueeze(1)\nx = x.to(torch.float32)\n\n# Cast the target labels to the 'long' data type\ny = y.to(torch.long)\n\n# Split data into train & valid set\nfrom sklearn.model_selection import train_test_split\n\nvalid_pct = .2\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size = valid_pct)\n\n# Normalization\ndef min_max_scale(data, a=0, b=1):\n    # Calculate the minimum and maximum values of the data\n    data_min = data.min()\n    data_max = data.max()\n\n    # Perform Min-Max Scaling\n    data_norm = (data - data_min) / (data_max - data_min)\n\n    # Rescale the data to the [a, b] range\n    data_norm = a + data_norm * (b - a)\n\n    return data_norm\n\nx_train_norm = min_max_scale(x_train)\nx_valid_norm = min_max_scale(x_valid)\n\n# Load data\nfrom torch.utils.data import TensorDataset, DataLoader\n\ndef load_data(x_tensor, y_tensor, batch_size, test):\n    data = TensorDataset(x_tensor, y_tensor)\n\n    data_loader = DataLoader(data, batch_size=batch_size, shuffle=not test, drop_last=not test)\n\n    print(f\"Total Mini-Batches: {len(data_loader)}\")\n    for i, (x, y) in enumerate(data_loader):\n        if i == 0:\n            print(f\"Shape of Each Mini-Batch: {x.shape}\")\n            print(\"\")\n            break\n    return data_loader\n\nbatch_size = 128\ntrain_loader = load_data(x_train_norm, y_train, batch_size=batch_size, test = False)\nvalid_loader = load_data(x_valid_norm, y_valid, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 375\nShape of Each Mini-Batch: torch.Size([128, 1, 28, 28])\n\nTotal Mini-Batches: 94\nShape of Each Mini-Batch: torch.Size([128, 1, 28, 28])"
  },
  {
    "objectID": "finetuning.html#resnet-18",
    "href": "finetuning.html#resnet-18",
    "title": "Fine Tuning",
    "section": "Resnet 18",
    "text": "Resnet 18\n\nimport torchvision.models as models\nresnet18 = models.resnet18(pretrained = True)\n\n\nresnet18\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=512, out_features=1000, bias=True)\n)\n\n\n\nimport torch.nn as nn\nprint(f\"Input Before: {resnet18.conv1}\\n\")\n\nresnet18.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n\nprint(f\"Input After: {resnet18.conv1}\")\n\nInput Before: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\nInput After: Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n\n\n\nprint(f\"Output Before: {resnet18.fc}\\n\")\n\nin_features = resnet18.fc.in_features\nresnet18.fc = nn.Linear(in_features, 10)\n\nprint(f\"Output After: {resnet18.fc}\")\n\nOutput Before: Linear(in_features=512, out_features=1000, bias=True)\n\nOutput After: Linear(in_features=512, out_features=10, bias=True)\n\n\n\ndef train_model(model, train_loader, valid_loader, \\\n                lr, n_epochs, device, weight_decay):\n\n    loss_fn = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay = weight_decay)\n\n    # Lists to store losses and training accuracy\n    losses = torch.zeros(n_epochs, len(train_loader))\n    train_acc = torch.zeros(n_epochs, len(train_loader))\n    valid_acc = torch.zeros(n_epochs, len(valid_loader))\n\n    for epoch in range(n_epochs):\n        for i, (x, y) in enumerate(train_loader):\n\n            # Set Model to train mode\n            model.train()\n\n            out = model(x.to(device))\n\n            # Calculate loss\n            loss = loss_fn(out, y.to(device))\n\n            # Backpropagation\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # Store the loss\n            losses[epoch, i] = loss.item()\n\n            # Calculate training accuracy\n            acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n            train_acc[epoch, i] = acc\n\n            # Set Model to evaluation mode\n            model.eval()\n\n            # Evaluation on validation data\n            with torch.no_grad():\n                for j, (x, y) in enumerate(valid_loader):\n\n                    out = model(x.to(device))\n\n                    acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n                    valid_acc[epoch, j] = acc\n\n    return model, losses, train_acc, valid_acc\n\n\nimport time\n# Get the start time\nstart_time = time.time()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nn_epochs = 5\ndroput_rate = .1\nweight_decay = .0001\nlearning_rate = .01\n\nmodel = resnet18.to(device)\n\nparameters, losses, train_acc, valid_acc = train_model(model, \\\n                                                             train_loader, \\\n                                                             valid_loader, \\\n                                                             learning_rate, \\\n                                                             n_epochs,\\\n                                                             device, \\\n                                                             weight_decay)\n\n# Get the end time\nend_time = time.time()\n\n# Calculate the execution time\nexecution_time = end_time - start_time\n\n# Convert to minutes and seconds\nexecution_time_minutes = int(execution_time // 60)\nexecution_time_seconds = round(execution_time % 60, 2)\n\nprint(f\"Model training time: {execution_time_minutes} min {execution_time_seconds}s\")\n\nModel training time: 19 min 5.14s\n\n\n\nimport matplotlib.pyplot as plt\n\nfinal_loss = losses.mean(1)[-1]\nfinal_train_acc = train_acc.mean(1)[-1]\nfinal_valid_acc = valid_acc.mean(1)[-1]\n\nfig, axs = plt.subplots(1, 2, figsize = (14, 3))\n\naxs[0].plot(range(losses.shape[0]), losses.mean(1), \"-o\")\naxs[0].set_title(f\"Train Loss is: {final_loss:.4f}\")\n\n\naxs[1].plot(range(train_acc.shape[0]), train_acc.mean(1), \"-o\")\naxs[1].plot(range(valid_acc.shape[0]), valid_acc.mean(1), \"-o\")\naxs[1].set_title(f\"Train: {final_train_acc:.2f}%, Valid: {final_valid_acc:.2f}%\")\naxs[1].legend([\"Train\", \"Valid\"])\n\nplt.suptitle(f\"Fine Tuning with Resnet 18\", fontsize = 16)\nplt.show()\n\n\n\n\n\nx_test = test_dataset.data\ny_test = test_dataset.targets\n\n# Reshape and cast the input data\nx_test = x_test.unsqueeze(1)\nx_test = x_test.to(torch.float32)\nx_test_norm = min_max_scale(x_test)\n\n# Cast the target labels to the 'long' data type\ny_test = y_test.to(torch.long)\n\ntest_loader = load_data(x_test_norm, y_test, batch_size=batch_size, test=True)\n\nTotal Mini-Batches: 79\nShape of Each Mini-Batch: torch.Size([128, 1, 28, 28])\n\n\n\n\ntest_acc = []\nmodel.eval()\nfor i, (x, y) in enumerate(test_loader):\n    out = model.forward(x.to(device))\n\n    acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n    test_acc.append(acc.cpu())\nmodel.train()\n\nprint(f\"Accuracy is: {np.mean(test_acc):.2f}%\")\n\nAccuracy is: 87.25%"
  },
  {
    "objectID": "finetuning.html#save-model",
    "href": "finetuning.html#save-model",
    "title": "Fine Tuning",
    "section": "Save Model",
    "text": "Save Model\n\n# Save Model\nfrom google.colab import drive\ndrive.mount('/content/drive')\nmodel_path = '/content/drive/MyDrive/fnmnist_model.pth'\ntorch.save(model.state_dict(), model_path)\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\n# Load the model\nmodel = models.resnet18(pretrained = False)\nmodel.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nin_features = model.fc.in_features\nmodel.fc = nn.Linear(in_features, 10)\n\nmodel.load_state_dict(torch.load('/content/drive/MyDrive/fnmnist_model.pth',  map_location=torch.device('cpu')))\n\n&lt;All keys matched successfully&gt;\n\n\n\ntest_acc = []\nmodel.to(device)\nmodel.eval()\nfor i, (x, y) in enumerate(test_loader):\n    out = model.forward(x.to(device))\n\n    acc = 100 * (out.argmax(1) == y.to(device)).float().mean()\n    test_acc.append(acc.cpu())\nmodel.train()\n\nprint(f\"Accuracy is: {np.mean(test_acc):.2f}%\")\n\nAccuracy is: 87.25%\n\n\n\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\n\nfor ax in axs.flatten():\n    # Select a random index from the output\n    random_index = np.random.choice(len(out))\n\n    # Display the image at the selected index with colormap gray for correct predictions\n    true_label = y_test[random_index]\n    guess_label = out.argmax(1)[random_index]\n\n    if true_label == guess_label:\n        ax.imshow(x_test[random_index].view(28, 28), cmap=\"gray\")\n    else:\n        ax.imshow(x_test[random_index].view(28, 28))\n\n\n    ax.set_title(f\"Guess: {classes[guess_label]}, True: {classes[true_label]}\")\n    ax.axis(\"off\")\n\nplt.suptitle(\"Result\", fontsize=16)\nplt.show()"
  }
]